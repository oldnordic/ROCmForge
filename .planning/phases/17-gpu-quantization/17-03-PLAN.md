---
phase: 17-gpu-quantization
plan: 03
type: execute
wave: 2
depends_on: [17-01, 17-02]
files_modified:
  - src/ggml/hip_backend/ops/quantized_matmul.rs
  - src/loader/gguf.rs
  - tests/q_dequant_tests.rs
autonomous: true

must_haves:
  truths:
    - "Fused Q4_0 matmul kernel executes dequantization on-the-fly during matmul"
    - "Fused Q4_K matmul kernel executes dequantization on-the-fly during matmul"
    - "Fused Q6_K matmul kernel executes dequantization on-the-fly during matmul"
    - "Quantized matmul integration tests verify GPU results match CPU reference"
    - "CPU dequantization fallback is removed for GPU tensors (QUANT-06)"
  artifacts:
    - path: "src/ggml/hip_backend/ops/quantized_matmul.rs"
      provides: "Fused dequant+matmul kernel wrappers for Q4_0, Q4_K, Q6_K"
      exports: ["matmul_q4_0", "matmul_q4_k", "matmul_q6_k"]
      contains: "Q4_0_MATMUL_HSACO|Q4_K_MATMUL_HSACO|Q6_K_MATMUL_HSACO"
    - path: "tests/q_dequant_tests.rs"
      provides: "Integration tests for GPU quantized matmul"
      contains: "test_gpu_quantized_matmul"
  key_links:
    - from: "src/ggml/hip_backend/ops/quantized_matmul.rs"
      to: "Q4_0_MATMUL_HSACO, Q4_K_MATMUL_HSACO, Q6_K_MATMUL_HSACO env vars"
      via: "Kernel cache loads HSACO file paths"
      pattern: 'std::env::var\("Q4_0_MATMUL_HSACO"\)'
---

<objective>
Integrate fused dequantization+matmul kernels for Q4_0, Q4_K, Q6_K and verify correctness with integration tests.

Purpose: Fused matmul eliminates the intermediate FP32 weight buffer entirely (~17x memory bandwidth savings). Weights stay quantized on GPU and dequantize on-the-fly during dot product computation.

Output: Fused matmul kernel wrappers wired into execution path, integration tests verifying correctness, CPU dequant fallback removed.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/17-gpu-quantization/17-RESEARCH.md
@.planning/phases/17-gpu-quantization/17-01-PLAN.md
@.planning/phases/17-gpu-quantization/17-02-PLAN.md

# Source files to reference
@src/ggml/hip_backend/ops/quantized_matmul.rs
@kernels/q4_0_matmul.hip
@kernels/q4_k_matmul.hip
@kernels/q6_k_matmul.hip
@src/loader/gguf.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire fused Q4_0 matmul kernel into execution path</name>
  <files>src/ggml/hip_backend/ops/quantized_matmul.rs</files>
  <action>
Update the existing `matmul_q4_0_gpu()` function to be production-ready:

1. Remove `#[allow(dead_code)]` from Q4_0KernelCache, Q4_0_CACHE, get_or_init_q4_0_cache()

2. Update `matmul_q4_0_gpu()` to:
   - Take quantized weights as `&[u8]` (already does)
   - Upload quantized weights directly to GPU buffer (already does)
   - Calculate proper grid dims: use ceil(n_cols / TILE_SIZE_N) for grid.x
   - Launch kernel with correct parameters: activations_ptr, weights_q4_0_ptr, output_ptr, m, n_rows, n_cols
   - Call `backend.synchronize()` after launch
   - Return Result<(), HipError>

3. The function already follows the fused matmul pattern - verify it:
   - Allocates quantized weight buffer (not FP32)
   - Calls `launch_kernel_with_module_shared()` with kernel args
   - Uses shared memory optimization (USE_LDS=1)

4. Add public wrapper `pub fn matmul_q4_0()` that:
   - Takes backend, input buffer, quantized weights, output buffer, dimensions
   - Internally calls get_or_init_q4_0_cache() then matmul_q4_0_gpu()
   - Returns Result<(), String> for external callers

Pattern reference: The existing code at lines 200-280 already has most of this - just remove dead_code markers and add public wrapper.
  </action>
  <verify>
# Verify no dead_code markers on Q4_0 items
grep -n "allow(dead_code)" src/ggml/hip_backend/ops/quantized_matmul.rs | grep -i "q4_0"
# Verify public wrapper exists
grep -n "^pub fn matmul_q4_0\(" src/ggml/hip_backend/ops/quantized_matmul.rs
# Verify function uploads quantized data (not FP32)
grep -A 20 "fn matmul_q4_0_gpu" src/ggml/hip_backend/ops/quantized_matmul.rs | grep "allocate_buffer.*quantized\|copy_from_host.*quantized"
  </verify>
  <done>
matmul_q4_0() public wrapper exists, uploads quantized weights directly, calls fused kernel, no dead_code markers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire fused Q4_K and Q6_K matmul kernels into execution path</name>
  <files>src/ggml/hip_backend/ops/quantized_matmul.rs</files>
  <action>
Following the Q4_0 pattern, make Q4_K and Q6_K matmul production-ready:

1. Remove `#[allow(dead_code)]` from Q4_KKernelCache, Q4_K_CACHE, get_or_init_q4_k_cache()
2. Remove `#[allow(dead_code)]` from Q6_KKernelCache, Q6_K_CACHE, get_or_init_q6_k_cache()

3. For Q4_K (if matmul_q4_k_gpu exists):
   - Update to upload quantized weights directly
   - Calculate proper grid dims for Q4_K super-block layout
   - Add public wrapper `pub fn matmul_q4_k()`

4. For Q6_K (if matmul_q6_k_gpu exists):
   - Update to upload quantized weights directly
   - Calculate proper grid dims for Q6_K block layout
   - Add public wrapper `pub fn matmul_q6_k()`

5. If either kernel function doesn't exist yet, create it following the Q4_0 pattern:
   - Create kernel cache struct
   - Create get_or_init function loading from Q4_K_MATMUL_HSACO or Q6_K_MATMUL_HSACO
   - Create GPU kernel wrapper that uploads quantized data and launches fused kernel

Pattern reference: Q4_0 matmul structure at lines 200-280.
  </action>
  <verify>
# Verify no dead_code markers on Q4_K/Q6_K items
grep -n "allow(dead_code)" src/ggml/hip_backend/ops/quantized_matmul.rs | grep -E "q4_k|q6_k"
# Verify public wrappers exist
grep -n "^pub fn matmul_q4_k\(\)\|^pub fn matmul_q6_k\(\)" src/ggml/hip_backend/ops/quantized_matmul.rs
# Verify functions load from correct env vars
grep -n "Q4_K_MATMUL_HSACO\|Q6_K_MATMUL_HSACO" src/ggml/hip_backend/ops/quantized_matmul.rs
  </verify>
  <done>
matmul_q4_k() and matmul_q6_k() public wrappers exist, upload quantized weights directly, call fused kernels, no dead_code markers.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add GPU quantized matmul integration tests</name>
  <files>tests/q_dequant_tests.rs</files>
  <action>
Add integration tests for fused quantized matmul:

IMPORTANT: First verify the test file exists before modifying:
```bash
test -f tests/q_dequant_tests.rs && echo "File exists" || echo "File does not exist"
```

If file exists (from Plans 01-02), append to it. If not, create new file.

1. Create `tests/q_dequant_tests.rs` test module (modify existing file):

2. Add `test_gpu_quantized_matmul_q4_0()` that:
   - Creates test input: 1x4 FP32 activations
   - Creates test weights: 4x4 Q4_0 quantized matrix
   - Runs CPU reference: dequantize + standard matmul
   - Runs GPU fused: calls matmul_q4_0() with quantized weights
   - Compares results with tolerance 0.01
   - Verifies GPU matches CPU reference

3. Add similar tests for Q4_K and Q6_K if kernels are available

4. Use `#[cfg(feature = "rocm")]` and `#[ignore]` for GPU-dependent tests

5. Add helper function to create quantized test matrices

Pattern reference: Phase 15-07 GPU sampling tests for CPU vs GPU comparison pattern.
  </action>
  <verify>
# Verify GPU matmul tests exist
grep -n "test_gpu_quantized_matmul" tests/q_dequant_tests.rs
# Verify tests use matmul_q4_0, matmul_q4_k, matmul_q6_k
grep -E "matmul_q4_0\(|matmul_q4_k\(|matmul_q6_k\(" tests/q_dequant_tests.rs
# Verify tests compile
cargo test --features rocm test_gpu_quantized_matmul --ignored 2>&1 | head -30
  </verify>
  <done>
Integration tests exist for Q4_0, Q4_K, Q6_K fused matmul, verify GPU results match CPU reference, marked as ignored for manual GPU testing.
  </done>
</task>

<task type="auto">
  <name>Task 4: Remove CPU dequantization fallback for GPU tensors</name>
  <files>src/loader/gguf.rs</files>
  <action>
Remove the CPU dequantization fallback paths for Q4_0, Q4_K, Q6_K when loading to GPU:

1. In `load_tensor_to_gpu()`, for Q4_0, Q4_K, Q6_K match arms:
   - Remove the CPU fallback code blocks
   - Change error handling to return error immediately if GPU kernel fails
   - Add clear error message indicating GPU dequantization failed

2. The logic should be:
   - Try GPU dequant kernel
   - If kernel load fails or launch fails, return Err with message
   - NO fallback to CPU dequantization (that would defeat the purpose)

3. Document in comments: CPU fallback is removed for GPU tensors - if GPU fails, fail fast

4. Update any callers that might need to handle these errors

Rationale: If we're loading to GPU, we should use GPU. CPU fallback wastes memory bandwidth and adds complexity. Let the caller decide whether to use CPU tensors instead.

Pattern: See existing "CPU fallback" comments at lines 750-753 for Q4_0.
  </action>
  <verify>
# Verify Q4_0/Q4_K/Q6_K arms do NOT have CPU fallback
grep -A 20 "GgufTensorType::Q4_0" src/loader/gguf.rs | grep -E "dequantize_q4_0\(|self\.dequantize"
# If no matches, CPU fallback is removed
# Verify error handling returns Err on GPU failure
grep -A 20 "GgufTensorType::Q4_0" src/loader/gguf.rs | grep "return Err\|map_err"
  </verify>
  <done>
CPU dequantization fallback removed for Q4_0, Q4_K, Q6_K GPU tensor loading. GPU kernel failures return Err immediately with clear error messages.
  </done>
</task>

</tasks>

<verification>
1. cargo check passes with rocm feature
2. Public matmul_q4_0(), matmul_q4_k(), matmul_q6_k() functions exist
3. Fused matmul kernels upload quantized weights (not FP32)
4. Integration tests verify GPU results match CPU reference
5. CPU fallback code removed from loader GPU paths
6. No new compiler warnings
</verification>

<success_criteria>
1. Q4_0, Q4_K, Q6_K matmul use fused kernels (dequant on-the-fly)
2. Weights remain quantized on GPU (no intermediate FP32 buffer)
3. Integration tests pass (GPU matches CPU reference)
4. CPU dequant fallback removed from GPU tensor loading (QUANT-06 satisfied)
5. ~17x memory bandwidth reduction achieved for quantized matmul
</success_criteria>

<output>
After completion, create `.planning/phases/17-gpu-quantization/17-03-SUMMARY.md`
</output>
