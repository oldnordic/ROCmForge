---
phase: 17-gpu-quantization
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ggml/hip_backend/ops/q4_k_dequant.rs
  - src/ggml/hip_backend/ops/q6_k_dequant.rs
  - src/ggml/hip_backend/ops/mod.rs
  - src/loader/gguf.rs
  - tests/q_dequant_tests.rs
autonomous: true

must_haves:
  truths:
    - "Q4_K weights dequantize on GPU without CPU round-trip"
    - "Q6_K weights dequantize on GPU without CPU round-trip"
    - "Q4_K and Q6_K quantized weights are uploaded directly to GPU (not dequantized first)"
    - "GPU dequantization produces bit-exact results matching CPU reference (QUANT-05)"
    - "CPU fallback path still works when GPU kernel is unavailable"
  artifacts:
    - path: "src/ggml/hip_backend/ops/q4_k_dequant.rs"
      provides: "GPU kernel invocation wrapper for Q4_K dequantization"
      exports: ["dequantize_q4_k_gpu_kernel", "get_or_init_q4_k_dequant_cache"]
      contains: "Q4_K_DEQUANT_HSACO"
      min_lines: 100
    - path: "src/ggml/hip_backend/ops/q6_k_dequant.rs"
      provides: "GPU kernel invocation wrapper for Q6_K dequantization"
      exports: ["dequantize_q6_k_gpu_kernel", "get_or_init_q6_k_dequant_cache"]
      contains: "Q6_K_DEQUANT_HSACO"
      min_lines: 100
    - path: "tests/q_dequant_tests.rs"
      provides: "Automated unit tests for Q4_K and Q6_K GPU quantization kernels (QUANT-05)"
      contains: "test_gpu_q4_k_bit_exact|test_gpu_q6_k_bit_exact"
  key_links:
    - from: "src/loader/gguf.rs::load_tensor_to_gpu"
      to: "src/ggml/hip_backend/ops/q4_k_dequant.rs::dequantize_q4_k_gpu_kernel"
      via: "Match arm for GgufTensorType::Q4_K calls GPU dequant wrapper"
      pattern: "GgufTensorType::Q4_K.*=>.*dequantize_q4_k"
    - from: "src/loader/gguf.rs::load_tensor_to_gpu"
      to: "src/ggml/hip_backend/ops/q6_k_dequant.rs::dequantize_q6_k_gpu_kernel"
      via: "Match arm for GgufTensorType::Q6_K calls GPU dequant wrapper"
      pattern: "GgufTensorType::Q6_K.*=>.*dequantize_q6_k"
---

<objective>
Implement GPU-side Q4_K and Q6_K dequantization using existing HIP kernels, replacing CPU fallback in tensor loading path.

Purpose: Enable on-device Q4_K and Q6_K dequantization for K-quants formats (common in larger models). The kernels already exist - this is integration work following the Q4_0 pattern.

Output: Q4_K and Q6_K weights uploaded as quantized bytes, dequantized on GPU, cached as FP32 DeviceTensor.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/17-gpu-quantization/17-RESEARCH.md

# Source files to reference
@src/ggml/hip_backend/ops/q4_0_dequant.rs
@src/loader/gguf.rs
@kernels/q4_k_dequant.hip
@kernels/q6_k_dequant.hip
@build.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Q4_K GPU dequantization module</name>
  <files>src/ggml/hip_backend/ops/q4_k_dequant.rs</files>
  <action>
Create new file `src/ggml/hip_backend/ops/q4_k_dequant.rs` following the q4_0_dequant.rs pattern:

IMPORTANT: First verify the file doesn't already exist:
```bash
test ! -f src/ggml/hip_backend/ops/q4_k_dequant.rs || echo "File already exists, aborting"
```

1. Module header: GPU-side Q4_K dequantization with Q4_K format documentation
   - Super-block size: 256 elements (8 sub-blocks of 32)
   - Bytes per super-block: 256 (16 scales, 16 mins, 224 quants)
   - Dequantization: value = min + (quant * scale)

2. Create `Q4_KDequantCache` struct with module and kernel fields
3. Create `static Q4_K_DEQUANT_CACHE: Mutex<Option<Q4_KDequantCache>>`
4. Implement `get_or_init_q4_k_dequant_cache()`:
   - Loads from `Q4_K_DEQUANT_HSACO` env var
   - Gets kernel function "q4_k_to_fp32_kernel"
   - Returns error if env var not set or file not found

5. Implement `dequantize_q4_k_gpu_kernel()`:
   - Takes: backend, quantized_data (&[u8]), output (&HipBuffer), num_elements (usize)
   - Calculates grid dims based on super-blocks (256 elements per block)
   - Launches kernel with proper args
   - Calls `backend.synchronize()`

6. Create public wrapper `dequantize_q4_k_with_fallback()` that:
   - Calls `dequantize_q4_k_gpu_kernel()` first
   - On failure, falls back to CPU dequantization
   - Returns Result<(), String>

7. Keep CPU fallback function `dequantize_q4_k_cpu()` for reference tests

Pattern reference: Use the same structure as q4_0_dequant.rs but adapt for Q4_K super-block layout.
  </action>
  <verify>
# Verify file was created (not pre-existing)
test -f src/ggml/hip_backend/ops/q4_k_dequant.rs
# File exists and compiles
cargo check --features rocm 2>&1 | grep -E "q4_k_dequant|error" | head -20
# Verify cache and kernel function exist
grep -n "struct.*DequantCache\|get_or_init_q4_k_dequant_cache\|dequantize_q4_k_gpu_kernel\|dequantize_q4_k_with_fallback" src/ggml/hip_backend/ops/q4_k_dequant.rs
  </verify>
  <done>
q4_k_dequant.rs file created with cache struct, init function, GPU kernel wrapper, fallback wrapper, and CPU reference implementation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Q6_K GPU dequantization module</name>
  <files>src/ggml/hip_backend/ops/q6_k_dequant.rs</files>
  <action>
Create new file `src/ggml/hip_backend/ops/q6_k_dequant.rs` following the q4_k_dequant.rs pattern:

IMPORTANT: First verify the file doesn't already exist:
```bash
test ! -f src/ggml/hip_backend/ops/q6_k_dequant.rs || echo "File already exists, aborting"
```

1. Module header: GPU-side Q6_K dequantization with Q6_K format documentation
   - Block size: 256 elements
   - Bytes per block: 256 (32 scales, 192 quants, 32 padding)
   - Dequantization: value = signed_6bit * scale

2. Create `Q6_KDequantCache` struct with module and kernel fields
3. Create `static Q6_K_DEQUANT_CACHE: Mutex<Option<Q6_KDequantCache>>`
4. Implement `get_or_init_q6_k_dequant_cache()`:
   - Loads from `Q6_K_DEQUANT_HSACO` env var
   - Gets kernel function "q6_k_to_fp32_kernel"
   - Returns error if env var not set or file not found

5. Implement `dequantize_q6_k_gpu_kernel()`:
   - Takes: backend, quantized_data (&[u8]), output (&HipBuffer), num_elements (usize)
   - Calculates grid dims based on blocks (256 elements per block)
   - Launches kernel with proper args
   - Calls `backend.synchronize()`

6. Create public wrapper `dequantize_q6_k_with_fallback()` that:
   - Calls `dequantize_q6_k_gpu_kernel()` first
   - On failure, falls back to CPU dequantization
   - Returns Result<(), String>

7. Keep CPU fallback function `dequantize_q6_k_cpu()` for reference tests

Pattern reference: Use the same structure as q4_k_dequant.rs but adapt for Q6_K block layout.
  </action>
  <verify>
# Verify file was created (not pre-existing)
test -f src/ggml/hip_backend/ops/q6_k_dequant.rs
# File exists and compiles
cargo check --features rocm 2>&1 | grep -E "q6_k_dequant|error" | head -20
# Verify cache and kernel function exist
grep -n "struct.*DequantCache\|get_or_init_q6_k_dequant_cache\|dequantize_q6_k_gpu_kernel\|dequantize_q6_k_with_fallback" src/ggml/hip_backend/ops/q6_k_dequant.rs
  </verify>
  <done>
q6_k_dequant.rs file created with cache struct, init function, GPU kernel wrapper, fallback wrapper, and CPU reference implementation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire Q4_K and Q6_K GPU dequantization into GgufLoader</name>
  <files>src/loader/gguf.rs</files>
  <action>
Modify `load_tensor_to_gpu()` to use GPU dequantization for Q4_K and Q6_K:

1. Import GPU dequant functions:
   ```rust
   use crate::ggml::hip_backend::ops::q4_k_dequant::dequantize_q4_k_with_fallback;
   use crate::ggml::hip_backend::ops::q6_k_dequant::dequantize_q6_k_with_fallback;
   ```

2. In the Q4_K match arm (around line 788):
   - Replace CPU dequantization with GPU kernel call
   - Call `dequantize_q4_k_with_fallback(backend, tensor_bytes, &output_buffer, num_elements)`
   - The wrapper function handles GPU kernel with CPU fallback internally

3. In the Q6_K match arm (around line 799):
   - Replace CPU dequantization with GPU kernel call
   - Call `dequantize_q6_k_with_fallback(backend, tensor_bytes, &output_buffer, num_elements)`
   - The wrapper function handles GPU kernel with CPU fallback internally

4. Both paths should:
   - Allocate output buffer
   - Call GPU dequant wrapper with quantized bytes directly
   - Synchronize and wrap in DeviceTensor
   - Cache result

Reference: The existing CPU dequantization paths at lines 788-810.
  </action>
  <verify>
# Verify Q4_K and Q6_K arms call GPU kernels
grep -A 5 "GgufTensorType::Q4_K" src/loader/gguf.rs | grep -E "dequantize_q4_k_with_fallback"
grep -A 5 "GgufTensorType::Q6_K" src/loader/gguf.rs | grep -E "dequantize_q6_k_with_fallback"
# Verify imports exist
grep -E "q4_k_dequant::dequantize|q6_k_dequant::dequantize" src/loader/gguf.rs
  </verify>
  <done>
Q4_K and Q6_K match arms call respective GPU fallback wrappers, upload quantized bytes directly, fallback handled by wrapper functions.
  </done>
</task>

<task type="auto">
  <name>Task 4: Update mod.rs to export Q4_K and Q6_K dequant functions</name>
  <files>src/ggml/hip_backend/ops/mod.rs</files>
  <action>
Update src/ggml/hip_backend/ops/mod.rs to declare and export the new Q4_K and Q6_K dequantization modules:

1. Add module declarations after q4_0_dequant (around line 11):
```rust
pub mod q4_k_dequant;
pub mod q6_k_dequant;
```

2. Add public exports:
```rust
pub use q4_k_dequant::{dequantize_q4_k_with_fallback, dequantize_q4_k_gpu_kernel, get_or_init_q4_k_dequant_cache};
pub use q6_k_dequant::{dequantize_q6_k_with_fallback, dequantize_q6_k_gpu_kernel, get_or_init_q6_k_dequant_cache};
```

This allows external modules (like gguf.rs) to import and use the GPU dequantization functions.
  </action>
  <verify>
# Verify module declarations exist
grep -n "pub mod q4_k_dequant\|pub mod q6_k_dequant" src/ggml/hip_backend/ops/mod.rs
# Verify exports exist
grep "pub use q4_k_dequant::\|pub use q6_k_dequant::" src/ggml/hip_backend/ops/mod.rs
# Verify specific functions exported
grep "dequantize_q4_k_with_fallback\|dequantize_q6_k_with_fallback" src/ggml/hip_backend/ops/mod.rs
  </verify>
  <done>
mod.rs declares q4_k_dequant and q6_k_dequant modules and exports their public functions for external use.
  </done>
</task>

<task type="auto">
  <name>Task 5: Add bit-exact unit tests for Q4_K and Q6_K (QUANT-05)</name>
  <files>tests/q_dequant_tests.rs</files>
  <action>
Add automated unit tests for Q4_K and Q6_K GPU dequantization that verify bit-exact outputs (QUANT-05 requirement).

IMPORTANT: The test file may already exist from Plan 01. First check:
```bash
test -f tests/q_dequant_tests.rs && echo "File exists" || echo "File does not exist"
```

If file exists, append to it. If not, create new file.

Add the following tests to `tests/q_dequant_tests.rs`:

```rust
#[test]
#[cfg(feature = "rocm")]
fn test_gpu_q4_k_bit_exact() {
    // QUANT-05: Verify Q4_K GPU dequantization produces bit-exact results
    let backend = match HipBackend::new() {
        Ok(b) => b,
        Err(_) => {
            println!("GPU not available - skipping test (not a failure)");
            return;
        }
    };

    // Create test data: 1 super-block with varying scales and values
    let mut data = vec![0u8; 256];  // Q4_K super-block size
    // Set scales (16 f32 values at offset 0)
    for i in 0..16 {
        data[i * 4..(i + 1) * 4].copy_from_slice(&(1.0f32.to_le_bytes()));
    }
    // Set mins (16 f32 values at offset 64)
    for i in 0..16 {
        data[64 + i * 4..68 + i * 4].copy_from_slice(&0.0f32.to_le_bytes());
    }
    // Set quantized values (224 bytes at offset 128)
    for i in 0..224 {
        data[128 + i] = ((i % 16) << 4) | (i % 16);
    }

    // CPU reference
    let cpu_result = dequantize_q4_k_cpu(&data, 256);

    // GPU result
    let output = backend.allocate_buffer(256 * 4).expect("Failed to allocate");
    dequantize_q4_k_gpu_kernel(&backend, &data, &output, 256)
        .expect("GPU dequant failed");
    backend.synchronize().expect("Sync failed");

    let mut gpu_result = vec![0.0f32; 256];
    output.copy_to_host(&mut gpu_result).expect("Copy to host failed");

    // Verify bit-exact match (tolerance 0.001 allows for minimal FP rounding)
    for i in 0..256 {
        let diff = (cpu_result[i] - gpu_result[i]).abs();
        assert!(diff < 0.001,
            "Q4_K mismatch at {}: CPU={}, GPU={}, diff={}",
            i, cpu_result[i], gpu_result[i], diff);
    }
}

#[test]
#[cfg(feature = "rocm")]
fn test_gpu_q6_k_bit_exact() {
    // QUANT-05: Verify Q6_K GPU dequantization produces bit-exact results
    let backend = match HipBackend::new() {
        Ok(b) => b,
        Err(_) => {
            println!("GPU not available - skipping test (not a failure)");
            return;
        }
    };

    // Create test data: 1 block with Q6_K format
    let mut data = vec![0u8; 256];  // Q6_K block size
    // Set scales (32 f32 values at offset 0)
    for i in 0..32 {
        data[i * 4..(i + 1) * 4].copy_from_slice(&(1.0f32.to_le_bytes()));
    }
    // Set quantized values (192 bytes at offset 128)
    for i in 0..192 {
        data[128 + i] = (i % 64) * 4;  // Valid 6-bit values
    }

    // CPU reference
    let cpu_result = dequantize_q6_k_cpu(&data, 256);

    // GPU result
    let output = backend.allocate_buffer(256 * 4).expect("Failed to allocate");
    dequantize_q6_k_gpu_kernel(&backend, &data, &output, 256)
        .expect("GPU dequant failed");
    backend.synchronize().expect("Sync failed");

    let mut gpu_result = vec![0.0f32; 256];
    output.copy_to_host(&mut gpu_result).expect("Copy to host failed");

    // Verify bit-exact match (tolerance 0.001 allows for minimal FP rounding)
    for i in 0..256 {
        let diff = (cpu_result[i] - gpu_result[i]).abs();
        assert!(diff < 0.001,
            "Q6_K mismatch at {}: CPU={}, GPU={}, diff={}",
            i, cpu_result[i], gpu_result[i], diff);
    }
}
```

Import the CPU reference functions:
```rust
use rocmforge::ggml::hip_backend::ops::q4_k_dequant::{dequantize_q4_k_gpu_kernel, dequantize_q4_k_cpu};
use rocmforge::ggml::hip_backend::ops::q6_k_dequant::{dequantize_q6_k_gpu_kernel, dequantize_q6_k_cpu};
```

Key pattern: Uses runtime check (HipBackend::new()) instead of #[ignore], runs in CI when GPU available.
  </action>
  <verify>
# Verify Q4_K test exists and is NOT #[ignore]
grep -n "test_gpu_q4_k_bit_exact" tests/q_dequant_tests.rs | grep -v "ignore"
# Verify Q6_K test exists and is NOT #[ignore]
grep -n "test_gpu_q6_k_bit_exact" tests/q_dequant_tests.rs | grep -v "ignore"
# Verify tests compile
cargo test --features rocm test_gpu_q4_k_bit_exact 2>&1 | head -20
cargo test --features rocm test_gpu_q6_k_bit_exact 2>&1 | head -20
  </verify>
  <done>
tests/q_dequant_tests.rs has test_gpu_q4_k_bit_exact() and test_gpu_q6_k_bit_exact() that run in CI (not #[ignore]), verify bit-exact GPU output against CPU reference with tight tolerance (QUANT-05 satisfied for Q4_K and Q6_K).
  </done>
</task>

</tasks>

<verification>
1. cargo check passes with rocm feature
2. Q4_K_DEQUANT_HSACO and Q6_K_DEQUANT_HSACO env vars referenced
3. load_tensor_to_gpu() Q4_K and Q6_K arms call GPU kernel wrappers
4. Fallback to CPU dequantization preserved via wrapper functions
5. New modules added to mod.rs exports
6. Automated unit tests exist for Q4_K and Q6_K GPU kernel correctness (QUANT-05 satisfied)
</verification>

<success_criteria>
1. Q4_K weights uploaded as quantized bytes (not FP32)
2. Q6_K weights uploaded as quantized bytes (not FP32)
3. GPU kernels invoked for dequantization (no CPU round-trip in happy path)
4. CPU fallback still works when GPU unavailable (via wrappers)
5. Code compiles without new warnings
6. Automated bit-exact unit tests pass for Q4_K and Q6_K (QUANT-05)
</success_criteria>

<output>
After completion, create `.planning/phases/17-gpu-quantization/17-02-SUMMARY.md`
</output>
