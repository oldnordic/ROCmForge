---
phase: 17-gpu-quantization
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ggml/hip_backend/ops/q4_k_dequant.rs
  - src/ggml/hip_backend/ops/q6_k_dequant.rs
  - src/ggml/hip_backend/ops/mod.rs
  - src/loader/gguf.rs
autonomous: true

must_haves:
  truths:
    - "Q4_K dequantization kernel is loaded from HSACO and executed on GPU"
    - "Q6_K dequantization kernel is loaded from HSACO and executed on GPU"
    - "Q4_K and Q6_K quantized weights are uploaded directly to GPU"
    - "GPU dequantization produces bit-exact results matching CPU reference"
  artifacts:
    - path: "src/ggml/hip_backend/ops/q4_k_dequant.rs"
      provides: "GPU kernel invocation wrapper for Q4_K dequantization"
      exports: ["dequantize_q4_k_gpu_kernel", "get_or_init_q4_k_dequant_cache"]
      contains: "Q4_K_DEQUANT_HSACO"
    - path: "src/ggml/hip_backend/ops/q6_k_dequant.rs"
      provides: "GPU kernel invocation wrapper for Q6_K dequantization"
      exports: ["dequantize_q6_k_gpu_kernel", "get_or_init_q6_k_dequant_cache"]
      contains: "Q6_K_DEQUANT_HSACO"
  key_links:
    - from: "src/loader/gguf.rs::load_tensor_to_gpu"
      to: "src/ggml/hip_backend/ops/q4_k_dequant.rs::dequantize_q4_k_gpu_kernel"
      via: "Match arm for GgufTensorType::Q4_K calls GPU dequant wrapper"
      pattern: "GgufTensorType::Q4_K.*=>.*dequantize_q4_k_gpu_kernel"
    - from: "src/loader/gguf.rs::load_tensor_to_gpu"
      to: "src/ggml/hip_backend/ops/q6_k_dequant.rs::dequantize_q6_k_gpu_kernel"
      via: "Match arm for GgufTensorType::Q6_K calls GPU dequant wrapper"
      pattern: "GgufTensorType::Q6_K.*=>.*dequantize_q6_k_gpu_kernel"
---

<objective>
Implement GPU-side Q4_K and Q6_K dequantization using existing HIP kernels, replacing CPU fallback in tensor loading path.

Purpose: Enable on-device Q4_K and Q6_K dequantization for K-quants formats (common in larger models). The kernels already exist - this is integration work following the Q4_0 pattern.

Output: Q4_K and Q6_K weights uploaded as quantized bytes, dequantized on GPU, cached as FP32 DeviceTensor.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/17-gpu-quantization/17-RESEARCH.md

# Source files to reference
@src/ggml/hip_backend/ops/q4_0_dequant.rs
@src/loader/gguf.rs
@kernels/q4_k_dequant.hip
@kernels/q6_k_dequant.hip
@build.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Q4_K GPU dequantization module</name>
  <files>src/ggml/hip_backend/ops/q4_k_dequant.rs</files>
  <action>
Create new file `src/ggml/hip_backend/ops/q4_k_dequant.rs` following the q4_0_dequant.rs pattern:

IMPORTANT: First verify the file doesn't already exist:
```bash
test ! -f src/ggml/hip_backend/ops/q4_k_dequant.rs || echo "File already exists, aborting"
```

1. Module header: GPU-side Q4_K dequantization with Q4_K format documentation
   - Super-block size: 256 elements (8 sub-blocks of 32)
   - Bytes per super-block: 256 (16 scales, 16 mins, 224 quants)
   - Dequantization: value = min + (quant * scale)

2. Create `Q4_KDequantCache` struct with module and kernel fields
3. Create `static Q4_K_DEQUANT_CACHE: Mutex<Option<Q4_KDequantCache>>`
4. Implement `get_or_init_q4_k_dequant_cache()`:
   - Loads from `Q4_K_DEQUANT_HSACO` env var
   - Gets kernel function "q4_k_to_fp32_kernel"
   - Returns error if env var not set or file not found

5. Implement `dequantize_q4_k_gpu_kernel()`:
   - Takes: backend, quantized_data (&[u8]), output (&HipBuffer), num_elements (usize)
   - Calculates grid dims based on super-blocks (256 elements per block)
   - Launches kernel with proper args
   - Calls `backend.synchronize()`

6. Create public wrapper `dequantize_q4_k_with_fallback()` that:
   - Calls `dequantize_q4_k_gpu_kernel()` first
   - On failure, falls back to CPU dequantization
   - Returns Result<(), String>

7. Keep CPU fallback function `dequantize_q4_k_cpu()` for reference tests

Pattern reference: Use the same structure as q4_0_dequant.rs but adapt for Q4_K super-block layout.
  </action>
  <verify>
# Verify file was created (not pre-existing)
test -f src/ggml/hip_backend/ops/q4_k_dequant.rs
# File exists and compiles
cargo check --features rocm 2>&1 | grep -E "q4_k_dequant|error" | head -20
# Verify cache and kernel function exist
grep -n "struct.*DequantCache\|get_or_init_q4_k_dequant_cache\|dequantize_q4_k_gpu_kernel\|dequantize_q4_k_with_fallback" src/ggml/hip_backend/ops/q4_k_dequant.rs
  </verify>
  <done>
q4_k_dequant.rs file created with cache struct, init function, GPU kernel wrapper, fallback wrapper, and CPU reference implementation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Q6_K GPU dequantization module</name>
  <files>src/ggml/hip_backend/ops/q6_k_dequant.rs</files>
  <action>
Create new file `src/ggml/hip_backend/ops/q6_k_dequant.rs` following the q4_k_dequant.rs pattern:

IMPORTANT: First verify the file doesn't already exist:
```bash
test ! -f src/ggml/hip_backend/ops/q6_k_dequant.rs || echo "File already exists, aborting"
```

1. Module header: GPU-side Q6_K dequantization with Q6_K format documentation
   - Block size: 256 elements
   - Bytes per block: 256 (32 scales, 192 quants, 32 padding)
   - Dequantization: value = signed_6bit * scale

2. Create `Q6_KDequantCache` struct with module and kernel fields
3. Create `static Q6_K_DEQUANT_CACHE: Mutex<Option<Q6_KDequantCache>>`
4. Implement `get_or_init_q6_k_dequant_cache()`:
   - Loads from `Q6_K_DEQUANT_HSACO` env var
   - Gets kernel function "q6_k_to_fp32_kernel"
   - Returns error if env var not set or file not found

5. Implement `dequantize_q6_k_gpu_kernel()`:
   - Takes: backend, quantized_data (&[u8]), output (&HipBuffer), num_elements (usize)
   - Calculates grid dims based on blocks (256 elements per block)
   - Launches kernel with proper args
   - Calls `backend.synchronize()`

6. Create public wrapper `dequantize_q6_k_with_fallback()` that:
   - Calls `dequantize_q6_k_gpu_kernel()` first
   - On failure, falls back to CPU dequantization
   - Returns Result<(), String>

7. Keep CPU fallback function `dequantize_q6_k_cpu()` for reference tests

Pattern reference: Use the same structure as q4_k_dequant.rs but adapt for Q6_K block layout.
  </action>
  <verify>
# Verify file was created (not pre-existing)
test -f src/ggml/hip_backend/ops/q6_k_dequant.rs
# File exists and compiles
cargo check --features rocm 2>&1 | grep -E "q6_k_dequant|error" | head -20
# Verify cache and kernel function exist
grep -n "struct.*DequantCache\|get_or_init_q6_k_dequant_cache\|dequantize_q6_k_gpu_kernel\|dequantize_q6_k_with_fallback" src/ggml/hip_backend/ops/q6_k_dequant.rs
  </verify>
  <done>
q6_k_dequant.rs file created with cache struct, init function, GPU kernel wrapper, fallback wrapper, and CPU reference implementation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire Q4_K and Q6_K GPU dequantization into GgufLoader</name>
  <files>src/loader/gguf.rs</files>
  <action>
Modify `load_tensor_to_gpu()` to use GPU dequantization for Q4_K and Q6_K:

1. Import GPU dequant functions:
   ```rust
   use crate::ggml::hip_backend::ops::q4_k_dequant::dequantize_q4_k_with_fallback;
   use crate::ggml::hip_backend::ops::q6_k_dequant::dequantize_q6_k_with_fallback;
   ```

2. In the Q4_K match arm (around line 788):
   - Replace CPU dequantization with GPU kernel call
   - Call `dequantize_q4_k_with_fallback(backend, tensor_bytes, &output_buffer, num_elements)`
   - The wrapper function handles GPU kernel with CPU fallback internally

3. In the Q6_K match arm (around line 799):
   - Replace CPU dequantization with GPU kernel call
   - Call `dequantize_q6_k_with_fallback(backend, tensor_bytes, &output_buffer, num_elements)`
   - The wrapper function handles GPU kernel with CPU fallback internally

4. Both paths should:
   - Allocate output buffer
   - Call GPU dequant wrapper with quantized bytes directly
   - Synchronize and wrap in DeviceTensor
   - Cache result

Reference: The existing CPU dequantization paths at lines 788-810.
  </action>
  <verify>
# Verify Q4_K and Q6_K arms call GPU kernels
grep -A 5 "GgufTensorType::Q4_K" src/loader/gguf.rs | grep -E "dequantize_q4_k_with_fallback"
grep -A 5 "GgufTensorType::Q6_K" src/loader/gguf.rs | grep -E "dequantize_q6_k_with_fallback"
# Verify imports exist
grep -E "q4_k_dequant::dequantize|q6_k_dequant::dequantize" src/loader/gguf.rs
  </verify>
  <done>
Q4_K and Q6_K match arms call respective GPU fallback wrappers, upload quantized bytes directly, fallback handled by wrapper functions.
  </done>
</task>

<task type="auto">
  <name>Task 4: Update mod.rs to export Q4_K and Q6_K dequant functions</name>
  <files>src/ggml/hip_backend/ops/mod.rs</files>
  <action>
Update src/ggml/hip_backend/ops/mod.rs to declare and export the new Q4_K and Q6_K dequantization modules:

1. Add module declarations after q4_0_dequant (around line 11):
```rust
pub mod q4_k_dequant;
pub mod q6_k_dequant;
```

2. Add public exports:
```rust
pub use q4_k_dequant::{dequantize_q4_k_with_fallback, dequantize_q4_k_gpu_kernel, get_or_init_q4_k_dequant_cache};
pub use q6_k_dequant::{dequantize_q6_k_with_fallback, dequantize_q6_k_gpu_kernel, get_or_init_q6_k_dequant_cache};
```

This allows external modules (like gguf.rs) to import and use the GPU dequantization functions.
  </action>
  <verify>
# Verify module declarations exist
grep -n "pub mod q4_k_dequant\|pub mod q6_k_dequant" src/ggml/hip_backend/ops/mod.rs
# Verify exports exist
grep "pub use q4_k_dequant::\|pub use q6_k_dequant::" src/ggml/hip_backend/ops/mod.rs
# Verify specific functions exported
grep "dequantize_q4_k_with_fallback\|dequantize_q6_k_with_fallback" src/ggml/hip_backend/ops/mod.rs
  </verify>
  <done>
mod.rs declares q4_k_dequant and q6_k_dequant modules and exports their public functions for external use.
  </done>
</task>

</tasks>

<verification>
1. cargo check passes with rocm feature
2. Q4_K_DEQUANT_HSACO and Q6_K_DEQUANT_HSACO env vars referenced
3. load_tensor_to_gpu() Q4_K and Q6_K arms call GPU kernel wrappers
4. Fallback to CPU dequantization preserved via wrapper functions
5. New modules added to mod.rs exports
</verification>

<success_criteria>
1. Q4_K weights uploaded as quantized bytes (not FP32)
2. Q6_K weights uploaded as quantized bytes (not FP32)
3. GPU kernels invoked for dequantization (no CPU round-trip in happy path)
4. CPU fallback still works when GPU unavailable (via wrappers)
5. Code compiles without new warnings
</success_criteria>

<output>
After completion, create `.planning/phases/17-gpu-quantization/17-02-SUMMARY.md`
</output>
