---
phase: 04-cpu-simd-backend
plan: 03
type: execute
depends_on: ["04-02"]
files_modified: [src/attention/cpu.rs]
autonomous: true
---

<objective>
Implement SIMD for attention operations for optimized CPU fallback.

Purpose: Accelerate attention computation (softmax, QK^T, weighted value) using SIMD
Output: Working SIMD-optimized attention operations
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/04-cpu-simd-backend/04-02-PLAN.md
@src/attention/cpu.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement softmax with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement softmax function using std::simd:

    ```rust
    #[cfg(target_arch = "x86_64")]
    use std::simd::{f32x8, SimdFloat, SimdPartialOrd};
    #[cfg(target_arch = "aarch64")]
    use std::simd::{f32x4, SimdFloat, SimdPartialOrd};

    pub fn softmax_simd(logits: &[f32]) -> Vec<f32> {
        // Use SIMD to compute exp(logits - max) efficiently
        let max_val = logits.iter().fold(f32::NEG_INFINITY, |a, b| a.max(b));

        // Process with architecture-specific vector width
        #[cfg(target_arch = "x86_64")]
        const LANES: usize = 8;
        #[cfg(target_arch = "aarch64")]
        const LANES: usize = 4;
        #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
        const LANES: usize = 1;

        if LANES > 1 {
            let mut exp_vals = vec![0.0; logits.len()];
            for chunk in logits.chunks(LANES) {
                if chunk.len() == LANES {
                    #[cfg(target_arch = "x86_64")]
                    {
                        let vec = f32x8::from_slice(chunk);
                        let max_vec = f32x8::splat(max_val);
                        let exp_vec = simd_exp(vec - max_vec);
                        let exp_arr = exp_vec.to_array();
                        exp_vals[chunk.start()..chunk.start() + LANES].copy_from_slice(&exp_arr);
                    }
                    #[cfg(target_arch = "aarch64")]
                    {
                        let vec = f32x4::from_slice(chunk);
                        let max_vec = f32x4::splat(max_val);
                        let exp_vec = simd_exp(vec - max_vec);
                        let exp_arr = exp_vec.to_array();
                        exp_vals[chunk.start()..chunk.start() + LANES].copy_from_slice(&exp_arr);
                    }
                } else {
                    // Handle remaining elements
                    for (i, &val) in chunk.iter().enumerate() {
                        exp_vals[chunk.start() + i] = (val - max_val).exp();
                    }
                }
            }
            exp_vals
        } else {
            // Scalar fallback
            softmax_scalar(logits)
        }
    }

    #[cfg(target_arch = "x86_64")]
    fn simd_exp(x: f32x8) -> f32x8 {
        // Polynomial approximation for exp
        // exp(x) ≈ 1 + x + x²/2 + x³/6 + x⁴/24
        let x2 = x * x;
        let x3 = x2 * x;
        let x4 = x2 * x2;
        f32x8::splat(1.0) + x + x2 * f32x8::splat(0.5) + x3 * f32x8::splat(1.0/6.0) + x4 * f32x8::splat(1.0/24.0)
    }

    #[cfg(target_arch = "aarch64")]
    fn simd_exp(x: f32x4) -> f32x4 {
        let x2 = x * x;
        let x3 = x2 * x;
        let x4 = x2 * x2;
        f32x4::splat(1.0) + x + x2 * f32x4::splat(0.5) + x3 * f32x4::splat(1.0/6.0) + x4 * f32x4::splat(1.0/24.0)
    }

    fn softmax_scalar(logits: &[f32]) -> Vec<f32> {
        let max_val = logits.iter().fold(f32::NEG_INFINITY, |a, b| a.max(b));
        let exp_vals: Vec<f32> = logits.iter().map(|&x| (x - max_val).exp()).collect();
        let sum: f32 = exp_vals.iter().sum();
        let inv_sum = 1.0 / sum;
        exp_vals.iter().map(|&x| x * inv_sum).collect()
    }
    ```

    Key optimizations:
    - Vectorized exp computation (polynomial approximation)
    - Max computation using horizontal operations
    - Architecture-specific vector widths (f32x8 for AVX2, f32x4 for NEON)
    - Scalar fallback for unsupported architectures
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD softmax implemented</done>
</task>

<task type="auto">
  <name>Task 2: Implement QK^T with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement query-key transpose operation using std::simd:

    ```rust
    #[cfg(target_arch = "x86_64")]
    use std::simd::{f32x8, SimdFloat};
    #[cfg(target_arch = "aarch64")]
    use std::simd::{f32x4, SimdFloat};

    pub fn qk_t_simd(q: &[f32], k: &[f32], n: usize, k_dim: usize) -> Vec<f32> {
        // Q: [n×k_dim], K: [k_dim×n], output: [n×n]
        #[cfg(target_arch = "x86_64")]
        let tile_size = 8;
        #[cfg(target_arch = "aarch64")]
        let tile_size = 4;
        #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
        let tile_size = 1;

        if tile_size > 1 {
            qk_t_simd_impl(q, k, n, k_dim, tile_size)
        } else {
            qk_t_scalar(q, k, n, k_dim)
        }
    }

    #[cfg(any(target_arch = "x86_64", target_arch = "aarch64"))]
    fn qk_t_simd_impl(q: &[f32], k: &[f32], n: usize, k_dim: usize, tile_size: usize) -> Vec<f32> {
        let mut result = vec![0.0; n * n];

        for i in (0..n).step_by(tile_size) {
            for j in (0..n).step_by(tile_size) {
                #[cfg(target_arch = "x86_64")]
                let mut sum = f32x8::splat(0.0);
                #[cfg(target_arch = "aarch64")]
                let mut sum = f32x4::splat(0.0);

                for kk in (0..k_dim).step_by(tile_size) {
                    #[cfg(target_arch = "x86_64")]
                    {
                        let q_vec = f32x8::from_slice(&q[i * k_dim + kk..]);
                        let k_vec = f32x8::from_slice(&k[kk * n + j..]);
                        sum += q_vec * k_vec;
                    }
                    #[cfg(target_arch = "aarch64")]
                    {
                        let q_vec = f32x4::from_slice(&q[i * k_dim + kk..]);
                        let k_vec = f32x4::from_slice(&k[kk * n + j..]);
                        sum += q_vec * k_vec;
                    }
                }

                // Store result
                let result_arr = sum.to_array();
                for jj in 0..tile_size.min(n - j) {
                    result[i * n + j + jj] = result_arr[jj];
                }
            }
        }

        result
    }

    fn qk_t_scalar(q: &[f32], k: &[f32], n: usize, k_dim: usize) -> Vec<f32> {
        let mut result = vec![0.0; n * n];
        for i in 0..n {
            for j in 0..n {
                let mut sum = 0.0;
                for kk in 0..k_dim {
                    sum += q[i * k_dim + kk] * k[kk * n + j];
                }
                result[i * n + j] = sum;
            }
        }
        result
    }
    ```

    Notes:
    - This is the core attention operation
    - Tile size matches architecture vector width (8 for AVX2, 4 for NEON)
    - Handles remaining elements with scalar fallback
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD QK^T implemented</done>
</task>

<task type="auto">
  <name>Task 3: Implement weighted value with SIMD</name>
  <files>src/attention/cpu.rs</files>
  <action>
    Implement weighted value operation using std::simd:

    ```rust
    #[cfg(target_arch = "x86_64")]
    use std::simd::{f32x8, SimdFloat};
    #[cfg(target_arch = "aarch64")]
    use std::simd::{f32x4, SimdFloat};

    pub fn weighted_value_simd(
        value: &[f32],
        weight: &[f32],
        output: &mut [f32],
    ) -> Result<(), Box<dyn std::error::Error>> {
        // value: [n], weight: [n], output: [n]
        // Perform output[i] = value[i] * weight[i] using SIMD

        let n = value.len();
        if output.len() < n {
            return Err("Output buffer too small".into());
        }

        #[cfg(target_arch = "x86_64")]
        let tile_size = 8;
        #[cfg(target_arch = "aarch64")]
        let tile_size = 4;
        #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
        let tile_size = 1;

        if tile_size > 1 {
            for i in (0..n).step_by(tile_size) {
                if i + tile_size <= n {
                    #[cfg(target_arch = "x86_64")]
                    {
                        let v_vec = f32x8::from_slice(&value[i..i + tile_size]);
                        let w_vec = f32x8::from_slice(&weight[i..i + tile_size]);
                        let result_vec = v_vec * w_vec;
                        let result_arr = result_vec.to_array();
                        output[i..i + tile_size].copy_from_slice(&result_arr);
                    }
                    #[cfg(target_arch = "aarch64")]
                    {
                        let v_vec = f32x4::from_slice(&value[i..i + tile_size]);
                        let w_vec = f32x4::from_slice(&weight[i..i + tile_size]);
                        let result_vec = v_vec * w_vec;
                        let result_arr = result_vec.to_array();
                        output[i..i + tile_size].copy_from_slice(&result_arr);
                    }
                } else {
                    // Handle remaining elements
                    for (idx, (&v, &w)) in value[i..].iter().zip(weight[i..].iter()).enumerate() {
                        output[i + idx] = v * w;
                    }
                }
            }
        } else {
            // Scalar fallback
            for (i, (&v, &w)) in value.iter().zip(weight.iter()).enumerate() {
                output[i] = v * w;
            }
        }

        Ok(())
    }
    ```

    This operation is used for attention weight application.
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD weighted value implemented</done>
</task>

</tasks>

<verification>
- [ ] softmax_simd implemented with std::simd
- [ ] qk_t_simd implemented with std::simd
- [ ] weighted_value_simd implemented with std::simd
- [ ] cargo check passes
- [ ] All attention CPU tests pass
</verification>

<success_criteria>
- Attention operations use std::simd for acceleration
- Scalar fallback for edge cases
- No breaking changes to attention API
</success_criteria>

<output>
After completion, create `.planning/phases/04-cpu-simd-backend/04-03-SUMMARY.md`
</output>
