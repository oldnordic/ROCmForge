---
phase: 04-cpu-simd-backend
plan: 02
type: execute
depends_on: ["04-01"]
files_modified: [src/backend/cpu_backend.rs, src/tensor/matmul.rs, src/attention/cpu.rs]
autonomous: true
---

<objective>
Implement CPU backend trait with SIMD-accelerated matmul operations.

Purpose: Provide transparent CPU fallback with optimized SIMD for matmul
Output: Working CPU matmul implementation with packed_simd acceleration
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/04-cpu-simd-backend/04-01-PLAN.md
@src/backend/mod.rs
@src/ggml/cpu_backend.rs
@src/tensor/matmul.rs
@src/attention/cpu.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify std::simd support (no dependency needed)</name>
  <files>Cargo.toml</files>
  <action>
    std::simd is part of Rust 1.82+ standard library - no external dependency needed.

    Verify MSRV in Cargo.toml is at least 1.82:
    ```toml
    [package]
    name = "rocforge"
    version = "0.1.0"
    edition = "2021"
    rust-version = "1.82"  # Required for stable std::simd
    ```

    Add to src/lib.rs for SIMD imports:
    ```rust
    // std::simd imports - enabled via cfg(target_arch)
    #[cfg(target_arch = "x86_64")]
    use std::simd::f32x8;
    #[cfg(target_arch = "aarch64")]
    use std::simd::f32x4;
    ```
  </action>
  <verify>cargo check passes, rust-version >= 1.82</verify>
  <done>MSRV set to 1.82+, std::simd available</done>
</task>

<task type="auto">
  <name>Task 2: Create CPU backend trait with matmul signature</name>
  <files>src/backend/cpu_backend.rs</files>
   <action>
    Create a new CPU backend trait with matmul operation:

    ```rust
    use crate::tensor::matmul::{MatmulResult, MatmulError};
    use std::collections::HashMap;
    use crate::ggml::{GgmlBackend, GgmlError, GgmlResult, Op, TensorDesc, TensorId};

    // SIMD imports per architecture (std::simd)
    #[cfg(target_arch = "x86_64")]
    use std::simd::{f32x8, SimdFloat, SimdPartialOrd};
    #[cfg(target_arch = "aarch64")]
    use std::simd::{f32x4, SimdFloat, SimdPartialOrd};

    pub struct CpuBackend {
        tensors: HashMap<TensorId, (TensorDesc, Vec<f32>)>,
        simd_capable: bool,
    }

    impl CpuBackend {
        pub fn new() -> Self {
            // Detect CPU SIMD support at runtime
            #[cfg(target_arch = "x86_64")]
            let simd_capable = is_x86_feature_detected!("avx2");
            #[cfg(target_arch = "aarch64")]
            let simd_capable = is_aarch64_feature_detected!("neon");
            #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64"))]
            let simd_capable = false;

            Self {
                tensors: HashMap::new(),
                simd_capable,
            }
        }

        pub fn is_simd_capable(&self) -> bool {
            self.simd_capable
        }
    }

    impl GgmlBackend for CpuBackend {
        type Buffer = Vec<f32>;

        fn alloc(&mut self, desc: &TensorDesc) -> GgmlResult<()> {
            let buffer = vec![0.0; desc.element_count()];
            self.tensors.insert(desc.id, (desc.clone(), buffer));
            Ok(())
        }

        fn free(&mut self, id: TensorId) -> GgmlResult<()> {
            self.tensors.remove(&id);
            Ok(())
        }

        fn tensor_desc(&self, id: TensorId) -> Option<&TensorDesc> {
            self.tensors.get(&id).map(|(desc, _)| desc)
        }

        fn buffer(&self, id: TensorId) -> Option<&Self::Buffer> {
            self.tensors.get(&id).map(|(_, buf)| buf)
        }

        fn buffer_mut(&mut self, id: TensorId) -> Option<&mut Self::Buffer> {
            self.tensors.get_mut(&id).map(|(_, buf)| buf)
        }

        fn execute_op(
            &mut self,
            op: &Op,
            inputs: &[TensorId],
            outputs: &[TensorId],
        ) -> GgmlResult<()> {
            match op {
                Op::MatMul { .. } => {
                    self.execute_matmul(inputs, outputs)?;
                }
                _ => Err(GgmlError::Unimplemented(format!(
                    "CPU backend op not implemented: {:?}",
                    op
                ))),
            }
        }

        fn synchronize(&mut self) -> ggez::Result<()> {
            Ok(())
        }
    }
    ```

    Keep the implementation minimal for now - just the trait and stub methods.
  </action>
  <verify>cargo check passes</verify>
  <done>CPU backend trait created with matmul stub</done>
</task>

<task type="auto">
  <name>Task 3: Implement SIMD matmul kernel</name>
  <files>src/backend/cpu_backend.rs</files>
  <action>
    Implement the execute_matmul method using packed_simd:

    ```rust
    impl CpuBackend {
        fn execute_matmul(&mut self, inputs: &[TensorId], outputs: &[TensorId]) -> GgmlResult<()> {
            // Get input tensors
            let a_id = inputs.get(0).ok_or_else(|| GgmlError::TensorNotFound)?;
            let b_id = inputs.get(1).ok_or_else(|| GgllBackend::TensorNotFound)?;
            let c_id = outputs.get(0).ok_or_else(|| GgllBackend::TensorNotFound)?;

            let a = self.buffer(a_id).ok_or_else(|| GgllBackend::BufferNotFound)?;
            let b = self.buffer(b_id).ok_or_else(|| GgllBackend::BufferNotFound)?;

            // Get tensor descriptors for dimensions
            let a_desc = self.tensor_desc(a_id)?;
            let b_desc = self.tensor_desc(b_id)?;
            let c_desc = self.tensor_desc(c_id)?;

            // Extract dimensions
            let m = a_desc.dims[0];  // m×k
            let k = a_desc.dims[1];
            let n = b_desc.dims[1];  // k×n

            // Validate dimensions match
            if a_desc.dims != b_desc.dims {
                return Err(GgmlError::DimensionMismatch(format!(
                    "Matrix dimension mismatch: {:?} != {:?}",
                    a_desc.dims, b_desc.dims
                )));
            }

            // Perform SIMD-accelerated matmul
            let c = if self.simd_capable {
                self.matmul_simd_simd(a, b, m, n, k)?
            } else {
                self.matmul_scalar(a, b, m, n, k)?
            };

            // Store result
            let c_buf = self.buffer_mut(c_id).ok_or_else(|| GgmlBackend::BufferNotFound)?;
            c_buf.copy_from_slice(&c);

            Ok(())
        }

        fn matmul_simd_simd(&self, a: &[f32], b: &[f32], m: usize, n: usize, k: usize) -> GgmlResult<Vec<f32>> {
            // Use std::simd - different vector widths per architecture
            #[cfg(target_arch = "x86_64")]
            {
                self.matmul_simd_avx2(a, b, m, n, k)
            }
            #[cfg(target_arch = "aarch64")]
            {
                self.matmul_simd_neon(a, b, m, n, k)
            }
            #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
            {
                self.matmul_scalar(a, b, m, n, k)
            }
        }

        #[cfg(target_arch = "x86_64")]
        fn matmul_simd_avx2(&self, a: &[f32], b: &[f32], m: usize, n: usize, k: usize) -> GgmlResult<Vec<f32>> {
            use std::simd::f32x8;

            // Simple tiled matmul using 256-bit vectors (8 floats)
            const TILE_SIZE: usize = 8;

            let mut c = vec![0.0; m * n];

            for i in (0..m).step_by(TILE_SIZE) {
                for j in (0..n).step_by(TILE_SIZE) {
                    let mut sum = f32x8::splat(0.0);

                    for kk in (0..k).step_by(TILE_SIZE) {
                        let a_vec = f32x8::from_slice(&a[i * k + kk..]);
                        let b_vec = f32x8::from_slice(&b[kk * n + j..]);

                        sum += a_vec * b_vec;
                    }

                    // Store result
                    let result = sum.to_array();
                    for jj in 0..TILE_SIZE.min(n - j) {
                        c[i * n + j + jj] = result[jj];
                    }
                }
            }

            Ok(c)
        }

        #[cfg(target_arch = "aarch64")]
        fn matmul_simd_neon(&self, a: &[f32], b: &[f32], m: usize, n: usize, k: usize) -> GgmlResult<Vec<f32>> {
            use std::simd::f32x4;

            // Simple tiled matmul using 128-bit vectors (4 floats)
            const TILE_SIZE: usize = 4;

            let mut c = vec![0.0; m * n];

            for i in (0..m).step_by(TILE_SIZE) {
                for j in (0..n).step_by(TILE_SIZE) {
                    let mut sum = f32x4::splat(0.0);

                    for kk in (0..k).step_by(TILE_SIZE) {
                        let a_vec = f32x4::from_slice(&a[i * k + kk..]);
                        let b_vec = f32x4::from_slice(&b[kk * n + j..]);

                        sum += a_vec * b_vec;
                    }

                    // Store result
                    let result = sum.to_array();
                    for jj in 0..TILE_SIZE.min(n - j) {
                        c[i * n + j + jj] = result[jj];
                    }
                }
            }

            Ok(c)
        }

        fn matmul_scalar(&self, a: &[f32], b: &[f32], m: usize, n: usize, k: usize) -> GgmlResult<Vec<f32>> {
            // Fallback scalar implementation
            let mut c = vec![0.0; m * n];

            for i in 0..m {
                for j in 0..n {
                    let mut sum = 0.0;
                    for kk in 0..k {
                        sum += a[i * k + kk] * b[kk * n + j];
                    }
                    c[i * n + j] = sum;
                }
            }

            Ok(c)
        }
    }
    ```

    Key implementation details:
    - Use f32x4 for 128-bit SIMD (4 floats per vector)
    - Process tiles to fit in cache lines
    - Horizontal and inner products using vector operations
    - Scalar fallback for unsupported architectures
  </action>
  <verify>cargo check passes</verify>
  <done>SIMD matmul kernel implemented with fallback</done>
</task>

</tasks>

<verification>
- [ ] packed_simd added to Cargo.toml
- [ ] CpuBackend trait implemented with matmul signature
- [ ] SIMD matmul kernel implemented using packed_simd f32x4
- [ ] Scalar fallback implemented
- [ ] cargo check passes
- [ ] All tests pass
</verification>

<success_criteria>
- CPU backend implements GgmlBackend trait
- Matmul operation uses packed_simd f32x4 on supported architectures
- Fallback to scalar when SIMD unavailable
- No breaking changes to existing code
</success_criteria>

<output>
After completion, create `.planning/phases/04-cpu-simd-backend/04-02-SUMMARY.md`
</output>
