---
phase: 16-gpu-rope-implementation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/attention/rope_gpu_tests.rs
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Multi-head rotation is handled correctly on GPU (ROPE-02)"
    - "Position IDs are handled for sequences up to max_seq_len (ROPE-03)"
    - "Unit tests verify rotation correctness - GPU matches CPU (ROPE-05)"
  artifacts:
    - path: "src/attention/rope_gpu_tests.rs"
      provides: "CPU vs GPU comparison tests for RoPE"
      contains: "test_rope_gpu_|test_rope_cpu_reference_"
  key_links:
    - from: "src/attention/rope_gpu_tests.rs"
      to: "src/attention/rope.rs"
      via: "Tests call apply_rope_device() and compare to CPU"
      pattern: "apply_rope_device"
---

<objective>
Add comprehensive RoPE GPU tests and verify existing tests pass.

Purpose: Ensure GPU RoPE implementation produces identical results to CPU reference implementation. Add missing tests for long context positions and multi-head independent rotation.

Output: Comprehensive test suite verifying GPU matches CPU for all RoPE operations including long context and multi-head scenarios.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/16-gpu-rope-implementation/16-RESEARCH.md
@.planning/ROADMAP.md

@src/attention/rope_gpu_tests.rs
@src/attention/rope.rs
</context>

<tasks>

<task type="auto">
  <name>Verify existing RoPE GPU tests pass (ROPE-05)</name>
  <files>src/attention/rope_gpu_tests.rs</files>
  <action>
    Run the existing RoPE GPU test suite to verify current correctness:
    1. Read src/attention/rope_gpu_tests.rs to understand existing test coverage
    2. Run: cargo test --features rocm --lib rope_gpu 2>&1 | tee /tmp/rope_tests.log
    3. Verify all existing tests pass (test_rope_gpu_single_query, test_rope_gpu_batch_keys, etc.)
    4. Check that GPU results match CPU results within TEST_TOLERANCE=1e-5
    5. If any tests fail, diagnose and document the failure

    DO NOT modify existing tests - this is verification only.
  </action>
  <verify>
    # All rope_gpu tests pass
    grep -E "test result:.*passed" /tmp/rope_tests.log
    # Verify GPU tests ran
    grep "test_rope_gpu" /tmp/rope_tests.log
  </verify>
  <done>
    All existing rope_gpu_tests pass with GPU matching CPU results within tolerance
  </done>
</task>

<task type="auto">
  <name>Add long context position ID test (ROPE-03)</name>
  <files>src/attention/rope_gpu_tests.rs</files>
  <action>
    Add a test for position IDs > 2048 to verify long context handling:
    1. Create new test function test_rope_gpu_long_context_positions()
    2. Use RopeConfig with max_seq_len=4096 (simulating long context model)
    3. Test at positions 2048, 3000, 4095 to verify position scaling works
    4. Compare CPU vs GPU results for these positions
    5. Add to existing rope_gpu_tests.rs module after existing tests (around line 236)

    Use same TEST_TOLERANCE=1e-5 as existing tests. Follow existing test pattern:
    - get_backend_or_skip() for GPU availability
    - CPU reference computation via apply_rope_cpu()
    - GPU run via apply_rope_device()
    - Element-wise comparison with assert_relative_eq

    Test should verify that high position IDs produce correct rotation scaling (theta = position^(-2*i/dim)).
  </action>
  <verify>
    grep -n "test_rope_gpu_long_context_positions" src/attention/rope_gpu_tests.rs
    cargo test --features rocm --lib test_rope_gpu_long_context_positions
  </verify>
  <done>
    New test function verifies position IDs beyond 2048 work correctly on GPU, matching CPU reference
  </done>
</task>

<task type="auto">
  <name>Add multi-head independent rotation test (ROPE-02)</name>
  <files>src/attention/rope_gpu_tests.rs</files>
  <action>
    Add test for multi-head RoPE to verify each head gets correct independent rotation:
    1. Create test_rope_gpu_multi_head_independent_rotation()
    2. Use num_heads=8, head_dim=8, seq_len=4
    3. Create input where each head has distinct values (use identity matrix per head for clarity)
    4. Verify CPU and GPU produce identical results for ALL heads
    5. Check that head 0 rotation != head 1 rotation (proves heads are independent)
    6. Verify rotation varies by position within each head

    Add test after the long context test. Follow same pattern:
    - CPU reference via apply_rope_cpu_batched()
    - GPU run via apply_rope_device() with proper tensor reshaping
    - Element-wise compare entire tensor
    - Additional assertion: head 0 output != head 1 output at same position

    This ensures RoPE doesn't accidentally mix data between attention heads.
  </action>
  <verify>
    grep -n "test_rope_gpu_multi_head_independent_rotation" src/attention/rope_gpu_tests.rs
    cargo test --features rocm --lib test_rope_gpu_multi_head_independent_rotation
  </verify>
  <done>
    Multi-head test verifies each head receives correct independent rotation, with no cross-head contamination
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. All existing rope_gpu_tests pass
2. New long context position test passes (positions > 2048)
3. New multi-head independent rotation test passes
4. No test failures or skipped tests (except backend unavailable)

Run: cargo test --features rocm --lib rope_gpu 2>&1 | grep -E "(test result|running|test_rope_gpu)"
</verification>

<success_criteria>
1. All existing rope_gpu_tests pass (GPU matches CPU)
2. Long context test (test_rope_gpu_long_context_positions) passes
3. Multi-head test (test_rope_gpu_multi_head_independent_rotation) passes
4. Test suite covers ROPE-02, ROPE-03, ROPE-05 requirements
</success_criteria>

<output>
After completion, create `.planning/phases/16-gpu-rope-implementation/16-02-SUMMARY.md`
</output>
