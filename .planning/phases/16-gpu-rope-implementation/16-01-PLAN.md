---
phase: 16-gpu-rope-implementation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/attention/rope.rs
  - src/attention/kernels.rs
  - src/attention/rope_gpu_tests.rs
autonomous: true
user_setup: []

must_haves:
  truths:
    - "RoPE application runs on GPU for GPU tensors (ROPE-01)"
    - "Multi-head rotation is handled correctly on GPU (ROPE-02)"
    - "Position IDs are handled for sequences up to max_seq_len (ROPE-03)"
    - "RoPE kernel is compiled and loaded via build.rs (ROPE-04)"
    - "CPU-GPU transfer overhead is eliminated - no to_host_vec in GPU path (ROPE-06)"
  artifacts:
    - path: "kernels/rope.hip"
      provides: "Basic RoPE GPU kernel"
      min_lines: 70
    - path: "kernels/position_embeddings.hip"
      provides: "Q+K RoPE kernel (single launch)"
      min_lines: 90
    - path: "src/attention/rope.rs"
      provides: "apply_q_device() and apply_k_device() GPU methods"
      contains: "apply_q_device|apply_k_device"
    - path: "src/attention/kernels.rs"
      provides: "rope_gpu_kernel() and position_embeddings_gpu_kernel() wrappers"
      contains: "rope_gpu_kernel|position_embeddings_gpu_kernel"
  key_links:
    - from: "src/attention/rope.rs"
      to: "src/attention/kernels.rs"
      via: "apply_rope_device() calls rope_gpu_kernel()"
      pattern: "rope_gpu_kernel"
    - from: "src/model/glm_position.rs"
      to: "src/attention/kernels.rs"
      via: "apply_position_embeddings_device() calls position_embeddings_gpu_kernel()"
      pattern: "position_embeddings_gpu_kernel"
    - from: "src/model/execution_plan/execution_plan_src.rs"
      to: "src/model/glm_position.rs"
      via: "self_attention() calls apply_position_embeddings_device()"
      pattern: "apply_position_embeddings_device"
---

<objective>
Verify GPU RoPE kernel implementation satisfies all ROPE-01 through ROPE-06 requirements.

Purpose: Ensure Rotary Position Embeddings run on GPU, eliminating CPU-GPU transfer overhead per transformer layer. The infrastructure exists; this plan verifies correctness and adds missing test coverage.

Output: Verified GPU RoPE implementation with comprehensive tests confirming:
- GPU kernel is used for GPU tensors (no CPU round-trip)
- Multi-head rotation works correctly
- Position IDs are handled for long context
- Kernel is compiled and loaded correctly
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/16-gpu-rope-implementation/16-RESEARCH.md
@.planning/ROADMAP.md

@src/attention/rope.rs
@src/attention/kernels.rs
@src/attention/rope_gpu_tests.rs
@src/model/glm_position.rs
@kernels/rope.hip
@kernels/position_embeddings.hip
</context>

<tasks>

<task type="auto">
  <name>Verify RoPE kernel compilation and loading</name>
  <files>build.rs</files>
  <action>
    Verify that RoPE kernels are compiled and HSACO files are generated:
    1. Check build.rs lines 48-52 confirm rope.hip and position_embeddings.hip are in kernels list
    2. Confirm ROPE_HSACO and POSITION_EMBEDDINGS_HSACO env vars are set during build
    3. Run: cargo build --features rocm 2>&1 | grep -i "rope"
    4. Verify output shows "Compiled kernels/rope.hip" and "Compiled kernels/position_embeddings.hip"
    5. Check target/debug/build/*/out/ for rope_kernel.hsaco and position_embeddings_kernel.hsaco

    DO NOT modify build.rs - kernels are already in the compilation list. Just verify they compile.
  </action>
  <verify>ls target/debug/build/*/out/rope_kernel.hsaco target/debug/build/*/out/position_embeddings_kernel.hsaco 2>/dev/null && echo "RoPE HSACO files found"</verify>
  <done>RoPE kernel HSACO files exist in build output directory</done>
</task>

<task type="auto">
  <name>Add long context position ID test (ROPE-03)</name>
  <files>src/attention/rope_gpu_tests.rs</files>
  <action>
    Add a test for position IDs > 2048 to verify long context handling:
    1. Create new test function test_rope_gpu_long_context_positions()
    2. Use RopeConfig with max_seq_len=4096 (simulating long context model)
    3. Test at position 2048, 3000, 4095 to verify position scaling works
    4. Compare CPU vs GPU results for these positions
    5. Add to existing rope_gpu_tests.rs module after existing tests (around line 236)

    Use same TEST_TOLERANCE=1e-5 as existing tests. Follow existing test pattern (get_backend_or_skip, CPU reference, GPU run, compare).
  </action>
  <verify>grep -n "test_rope_gpu_long_context_positions" src/attention/rope_gpu_tests.rs</verify>
  <done>New test function verifies position IDs beyond 2048 work correctly on GPU</done>
</task>

<task type="auto">
  <name>Verify no CPU round-trip in GPU RoPE path (ROPE-06)</name>
  <files>src/attention/rope.rs</files>
  <action>
    Verify that apply_q_device() and apply_k_device() do NOT download to CPU:
    1. Read src/attention/rope.rs apply_rope_device() method (lines 226-327)
    2. Confirm NO to_host_vec() calls in the GPU path
    3. Confirm cos/sin are uploaded via from_host_vec() (required - these are computed on CPU)
    4. Verify the GPU kernel is called and synchronize() is called
    5. Document findings: The GPU path is pure - tensors stay on GPU

    If any to_host_vec() is found in the hot path, document it. The current implementation should be clean.
  </action>
  <verify>grep -n "to_host_vec" src/attention/rope.rs | grep -A2 -B2 "apply_rope_device"</verify>
  <done>Verified apply_rope_device() has no CPU round-trip - only cos/sin upload, then pure GPU execution</done>
</task>

<task type="auto">
  <name>Verify multi-head RoPE correctness (ROPE-02)</name>
  <files>src/attention/rope_gpu_tests.rs</files>
  <action>
    Add test for multi-head RoPE to verify each head gets correct rotation:
    1. Create test_rope_gpu_multi_head_independent_rotation()
    2. Use num_heads=8, head_dim=8, seq_len=4
    3. Create input where each head has distinct values
    4. Verify CPU and GPU produce identical results for ALL heads
    5. Check that head 0 rotation != head 1 rotation (heads are independent)

    Add test after existing tests. Follow same pattern: CPU reference, GPU run, element-wise compare.
  </action>
  <verify>grep -n "test_rope_gpu_multi_head_independent_rotation" src/attention/rope_gpu_tests.rs</verify>
  <done>Multi-head test verifies each head receives correct independent rotation</done>
</task>

<task type="auto">
  <name>Document RoPE GPU execution flow verification</name>
  <files>src/model/glm_position.rs</files>
  <action>
    Document the complete GPU RoPE execution flow:
    1. Trace from execution_plan_src.rs self_attention() line 1948: apply_position_embeddings_device()
    2. To glm_position.rs line 278-353: GPU kernel attempt with CPU fallback
    3. To kernels.rs line 1092-1148: position_embeddings_gpu_kernel()
    4. To position_embeddings.hip: actual HIP kernel execution
    5. Create inline comment in glm_position.rs documenting this flow at line 278

    Add comment block above apply_position_embeddings_device() explaining:
    - GPU kernel is tried first (position_embeddings_gpu_kernel)
    - Falls back to CPU only if kernel fails
    - For successful GPU execution, no CPU round-trip occurs
  </action>
  <verify>grep -n "GPU RoPE execution flow" src/model/glm_position.rs</verify>
  <done>Execution flow documented in code comments for maintainability</done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. RoPE HSACO files exist in build output (ROPE-04)
2. New long context test passes (ROPE-03)
3. Multi-head test passes (ROPE-02)
4. No to_host_vec() in GPU RoPE hot path (ROPE-06)
5. Documentation added explaining GPU flow (ROPE-01)
6. All existing rope_gpu_tests still pass

Run: cargo test --features rocm --lib rope 2>&1 | grep -E "(test result|running)"
</verification>

<success_criteria>
1. All RoPE kernels compile successfully (HSACO files present)
2. Long context position test (positions > 2048) passes
3. Multi-head independent rotation test passes
4. Code review confirms no CPU round-trip in GPU path
5. Execution flow documented in glm_position.rs
6. All rope_gpu_tests pass (GPU matches CPU)
</success_criteria>

<output>
After completion, create `.planning/phases/16-gpu-rope-implementation/16-01-SUMMARY.md`
</output>
