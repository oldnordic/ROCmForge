# Task 09-13: Profile Time to First Token (TTFT)

**Status:** Complete
**Date:** 2026-01-18
**Wave:** 4 - Latency Optimization
**Dependencies:** 09-01 (kernel timing), 09-06 (inference benchmark)

---

## Summary

Implemented comprehensive TTFT (Time to First Token) profiling infrastructure with detailed component breakdown analysis. TTFT is a critical latency metric for user experience in chat applications, representing the time from request submission to the first generated token appearing.

---

## TTFT Components

TTFT was broken down into the following measurable components:

| Component | Description | Target Contribution |
|-----------|-------------|---------------------|
| **Model Loading** | Time to load model weights into GPU memory | One-time cost (cached in production) |
| **Tokenization** | Converting text prompt to token IDs | ~0.01ms/token |
| **Embedding Lookup** | Looking up token embeddings | ~0.05ms/token |
| **Prompt Processing** | Processing prompt through all layers (prefill) | Dominant for long prompts |
| **First Token Generation** | LM head computation + sampling | ~10ms |
| **H2D Transfer** | CPU-to-GPU memory transfers | ~2ms |
| **D2H Transfer** | GPU-to-CPU memory transfers | ~1ms |

### Key Insight: Prompt Processing Dominance

For 512 token prompts, prompt processing dominates TTFT due to the O(n^2) complexity of the attention mechanism:
- 32 tokens: ~1ms (embedding + minimal attention)
- 128 tokens: ~9ms (quadratic scaling)
- 512 tokens: ~150ms (near target limit)

---

## Implementation

### 1. TTFT Profiling Module (`src/profiling/ttft.rs`)

Created a new profiling module with:

**TtftProfiler** - Simple API for measuring TTFT components:
```rust
let mut profiler = TtftProfiler::new();
profiler.start_ttft();

profiler.start_model_loading();
// ... load model ...
profiler.stop_model_loading();

profiler.start_prompt_processing();
// ... process prompt ...
profiler.stop_prompt_processing();

let breakdown = profiler.finish_ttft();
println!("{}", breakdown);
```

**TtftBreakdown** - Structured TTFT results:
- Component timings (model loading, tokenization, embedding, prompt processing, first token, transfers)
- Percentage calculations for bottleneck identification
- Target compliance checking (<200ms)
- Per-token metrics
- Dominant component detection

### 2. Inference Benchmark Enhancement (`benches/inference_bench.rs`)

Added TTFT-specific benchmarks:
- `benchmark_ttft_breakdown()` - Detailed component breakdown for multiple prompt lengths
- `benchmark_ttft_target_compliance()` - Specific test for 512-token target
- `simulate_prompt_processing()` - Models quadratic scaling of attention

### 3. KernelTimer Integration

The TTFT profiler can record kernel-level timings:
```rust
profiler.record_kernel("matmul", duration_ms, element_count);
profiler.record_kernel_from_timer("attention", &timer, count);
```

---

## TTFT Target Analysis

**Target:** TTFT < 200ms for 512 token prompts

### Synthetic Baseline Results

| Prompt Length | TTFT (ms) | Prompt Processing | % of TTFT | Target Status |
|---------------|-----------|-------------------|-----------|---------------|
| 32 tokens | ~18ms | ~1ms | 5.6% | PASS |
| 128 tokens | ~30ms | ~9ms | 30.0% | PASS |
| 512 tokens | ~173ms | ~150ms | 86.7% | PASS |

### Bottleneck Identification

For 512 token prompts:
- **Prompt processing** is the dominant component (86.7% of TTFT)
- Quadratic scaling due to attention mechanism: O(prompt_len^2)
- Per-token cost: ~0.29ms/token at 512 tokens

---

## Optimization Recommendations

### 1. Prompt Processing Optimization (Highest Priority)

**Impact:** 86.7% of TTFT for 512 tokens

Recommendations:
- Implement flash attention for reduced memory bandwidth
- Optimize attention kernel for batch processing (not single-token)
- Consider operator fusion (dequantization + matmul)
- Profile memory bandwidth utilization

### 2. Memory Transfer Optimization

**Impact:** ~1.7% of TTFT

Recommendations:
- Use pinned memory for faster transfers
- Overlap transfers with computation where possible
- Reduce CPU-GPU synchronization points

### 3. Model Loading (One-Time Cost)

**Impact:** Negligible after first request

Recommendations:
- Keep model resident in memory for multiple requests
- Use faster storage (NVMe vs SSD) for initial load
- Consider model weight caching

---

## Files Modified

### New Files
```
src/profiling/ttft.rs         - TTFT profiling module (600+ LOC)
.planning/phases/09-performance-optimization/09-13-TTFT.md - This document
```

### Modified Files
```
src/profiling/mod.rs          - Added ttft module and exports
benches/inference_bench.rs     - Added TTFT-specific benchmarks
src/prompt/cache.rs            - Fixed move-after-use bug (blocking compilation)
```

---

## API Documentation

### TtftProfiler

```rust
pub struct TtftProfiler {
    // Private fields
}

impl TtftProfiler {
    pub fn new() -> Self;
    pub fn start_ttft(&mut self);
    pub fn finish_ttft(&self) -> TtftBreakdown;

    // Component timing
    pub fn start_model_loading(&mut self);
    pub fn stop_model_loading(&mut self);
    pub fn start_tokenization(&mut self);
    pub fn stop_tokenization(&mut self);
    pub fn start_prompt_processing(&mut self);
    pub fn stop_prompt_processing(&mut self);
    pub fn start_first_token(&mut self);
    pub fn stop_first_token(&mut self);
    pub fn start_h2d_transfer(&mut self);
    pub fn stop_h2d_transfer(&mut self);
    pub fn start_d2h_transfer(&mut self);
    pub fn stop_d2h_transfer(&mut self);

    // Metadata
    pub fn set_prompt_token_count(&mut self, count: usize);
    pub fn set_quantization_format(&mut self, format: impl Into<String>);

    // Kernel-level timing
    pub fn record_kernel(&mut self, name: impl Into<String>, duration_ms: f64, element_count: usize);
}
```

### TtftBreakdown

```rust
pub struct TtftBreakdown {
    pub total_ttft_ms: f64,
    pub model_loading_ms: f64,
    pub tokenization_ms: f64,
    pub embedding_lookup_ms: f64,
    pub prompt_processing_ms: f64,
    pub first_token_ms: f64,
    pub h2d_transfer_ms: f64,
    pub d2h_transfer_ms: f64,
    pub prompt_token_count: usize,
    pub quantization_format: Option<String>,
    pub kernel_timings: Vec<KernelTiming>,
}

impl TtftBreakdown {
    pub fn model_loading_pct(&self) -> f64;
    pub fn prompt_processing_pct(&self) -> f64;
    pub fn memory_transfer_pct(&self) -> f64;
    pub fn prompt_processing_per_token_ms(&self) -> f64;
    pub fn dominant_component(&self) -> &'static str;
    pub fn meets_target(&self) -> bool;
    pub fn target_gap_ms(&self) -> f64;
    pub fn format_table(&self) -> String;
    pub fn optimization_summary(&self) -> String;
}
```

---

## Next Steps

### Task 09-14: Reduce Kernel Launch Overhead
- Profile kernel launch frequency and duration
- Batch small operations where possible
- Reduce CPU-GPU synchronization points

### Task 09-15: Optimize Prompt Processing Path
- Profile attention computation during prompt processing
- Optimize for batch processing (not single-token)
- Tune flash attention for prompt phase
- Reduce KV cache write overhead

---

## Testing

### Unit Tests (TTFT Module)
All TTFT module tests pass:
```
test_ttft_breakdown_new                        PASSED
test_ttft_breakdown_percentages                PASSED
test_ttft_breakdown_per_token                  PASSED
test_ttft_breakdown_dominant_component         PASSED
test_ttft_breakdown_target                     PASSED
test_ttft_profiler_new                         PASSED
test_ttft_profiler_phases                      PASSED
test_ttft_profiler_record_kernel               PASSED
test_create_ttft_breakdown                     PASSED
test_optimization_summary                      PASSED
```

### Benchmark Compilation
```
cargo build --bench inference_bench
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 5.23s
```

---

## Notes

- This task established the TTFT profiling infrastructure but did not measure actual GPU inference times
- Synthetic baselines are provided for compile-time verification
- Actual TTFT measurements require GPU hardware and real GGUF models
- The infrastructure is ready for integration with actual inference paths (tasks 09-14, 09-15)
