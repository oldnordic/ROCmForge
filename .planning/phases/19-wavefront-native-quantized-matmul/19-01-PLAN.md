---
phase: 19-wavefront-native-quantized-matmul
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: true

must_haves:
  truths:
    - "Documentation created: Q4_0/Q4_K/Q6_K format specifications documented in 19-01-SUMMARY.md"
    - "CPU reference implementations verified to match llama.cpp numerical specifications"
    - "Format bit-packing details recorded for GPU kernel implementation reference"
  artifacts:
    - path: ".planning/phases/19-wavefront-native-quantized-matmul/19-01-SUMMARY.md"
      provides: "Format specification documentation for all three quantization formats"
      min_lines: 50
      contains: "Q4_0 format|Q4_K format|Q6_K format|block size|bit packing"
    - path: "src/ggml/hip_backend/ops/q4_0_dequant.rs"
      provides: "CPU reference Q4_0 dequantization for validation"
      contains: "dequantize_q4_0_cpu"
    - path: "src/ggml/hip_backend/ops/q4_k_dequant.rs"
      provides: "CPU reference Q4_K dequantization for validation"
      contains: "dequantize_q4_k_cpu"
    - path: "src/ggml/hip_backend/ops/q6_k_dequant.rs"
      provides: "CPU reference Q6_K dequantization for validation"
      contains: "dequantize_q6_k_cpu"
  key_links:
    - from: "19-02-PLAN.md"
      to: "CPU reference implementations"
      via: "Uses numerical formulas as ground truth for GPU kernel validation"
      pattern: "dequantize_q4_0_cpu|dequantize_q4_k_cpu|dequantize_q6_k_cpu"
---

<objective>
Analyze quantization format data layouts and validate CPU reference implementations against mathematical specifications.

Purpose: Establish ground truth for quantization bit-packing before modifying GPU kernels. The CPU implementations in Phase 17 are the reference for numerical correctness. This plan documents the bit-level layouts to inform HIP-native kernel rewrites.

Output: Documented understanding of Q4_0, Q4_K, Q6_K formats with validation that CPU references match llama.cpp specifications.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/19-wavefront-native-quantized-matmul/19-RESEARCH.md
@.planning/research/ANTI_CUDA_PORTING_RATIONALE.md

@src/ggml/hip_backend/ops/q4_0_dequant.rs
@src/ggml/hip_backend/ops/q4_k_dequant.rs
@src/ggml/hip_backend/ops/q6_k_dequant.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document Q4_0 format bit-packing</name>
  <files>src/ggml/hip_backend/ops/q4_0_dequant.rs</files>
  <action>
    Read and document the Q4_0 format from q4_0_dequant.rs:
    - Block size: 32 elements in 20 bytes (4 bytes scale + 16 bytes packed data)
    - Bit packing: 2 values per byte (low nibble = even index, high nibble = odd index)
    - Dequantization formula: value = scale * ((packed & 0x0F) - 8)
    - Signed range: 0-15 unpacked to -8 to +7

    Verify against comment at top of file (lines 1-40). Create summary of bit-level layout for reference in 19-02.
  </action>
  <verify>
    grep -n "Q4_0_BLOCK_SIZE\|Q4_0_ELEMENTS_PER_BLOCK" /home/feanor/Projects/ROCmForge/src/ggml/hip_backend/ops/q4_0_dequant.rs
  </verify>
  <done>
    Q4_0 format documented: 32 elements/block, 20 bytes/block, scale + 16 bytes packed 4-bit values. Formula matches llama.cpp spec.
  </done>
</task>

<task type="auto">
  <name>Task 2: Document Q4_K format bit-packing</name>
  <files>src/ggml/hip_backend/ops/q4_k_dequant.rs</files>
  <action>
    Read and document the Q4_K format from q4_k_dequant.rs:
    - Super-block size: 256 elements in 256 bytes
    - 8 sub-blocks of 32 elements each
    - Per sub-block: scale (f16, 2 bytes) + min (int8, 1 byte) + 28 bytes packed 4-bit values
    - Dequantization formula: value = min + (quant * scale)
    - Bit extraction: 2 bytes combined, shift by bit_offset, mask with 0x0F

    Verify against comment at top of file (lines 1-27). Create summary of bit-level layout for reference in 19-02.
  </action>
  <verify>
    grep -n "Q4_K\|super-block\|sub-block" /home/feanor/Projects/ROCmForge/src/ggml/hip_backend/ops/q4_k_dequant.rs | head -20
  </verify>
  <done>
    Q4_K format documented: 256-element super-blocks, 8 sub-blocks, scale/min per sub-block, value = min + quant*scale.
  </done>
</task>

<task type="auto">
  <name>Task 3: Document Q6_K format bit-packing</name>
  <files>src/ggml/hip_backend/ops/q6_k_dequant.rs</files>
  <action>
    Read and document the Q6_K format from q6_k_dequant.rs:
    - Block size: 256 elements in 256 bytes
    - 16 half-precision scales (32 bytes) at block start
    - 192 bytes of 6-bit packed quantized values
    - 16 elements share one scale
    - Dequantization: signed_6bit * scale (signed: if >= 32, subtract 64)
    - Bit extraction: bit_pos = element * 6, extract 6 bits across byte boundaries

    Verify against comment at top of file (lines 1-18). Create summary of bit-level layout for reference in 19-02.
  </action>
  <verify>
    grep -n "Q6_K\|6-bit\|signed" /home/feanor/Projects/ROCmForge/src/ggml/hip_backend/ops/q6_k_dequant.rs | head -20
  </verify>
  <done>
    Q6_K format documented: 256-element blocks, 16 f16 scales, 6-bit packed values, signed conversion (>=32 subtract 64).
  </done>
</task>

</tasks>

<verification>
- All three format specifications documented with bit-level layout
- CPU implementations verified against comments/specifications in each file
- Documentation ready for use as reference in HIP kernel rewrites (19-02, 19-03)
</verification>

<success_criteria>
- Q4_0, Q4_K, Q6_K bit-packing formats fully documented in 19-01-SUMMARY.md
- CPU reference implementations confirmed as numerically correct ground truth
- No code changes (this is analysis/documentation only)
</success_criteria>

<output>
After completion, create `.planning/phases/19-wavefront-native-quantized-matmul/19-01-SUMMARY.md` with format documentation.
</output>
