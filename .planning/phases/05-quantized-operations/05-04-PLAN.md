---
phase: 05-quantized-operations
plan: 04
type: execute
depends_on: ["05-02", "05-03"]
files_modified: [kernels/q4_0_matmul.hip, build.rs, src/ggml/hip_backend/ops/quantized_matmul.rs]
autonomous: true
---

<objective>
Implement fused dequantization + matmul HIP kernel for Q4_0 format.

Purpose: Combine dequantization and matrix multiplication into a single GPU kernel, eliminating intermediate FP32 storage. This is critical for inference efficiency - we don't want to materialize full FP32 weights before matmul.
Output: Fused Q4_0 dequant-matmul HIP kernel with Rust wrapper.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/05-quantized-operations/05-02-SUMMARY.md
@.planning/phases/05-quantized-operations/05-03-SUMMARY.md
@src/ggml/hip_backend/ops/quantized_matmul.rs
@kernels/q4_0_dequant.hip
@kernels/qkt_matmul.hip
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create fused Q4_0 dequant-matmul HIP kernel</name>
  <files>kernels/q4_0_matmul.hip</files>
  <action>
    Create kernels/q4_0_matmul.hip for fused dequantization + matmul:

    Kernel specification:
    - Input: Activations (FP32, [M x K]), Weights (Q4_0 quantized, [K x N])
    - Output: FP32 result [M x N]
    - M: Batch/sequence dimension (typically 1 for single token)
    - N: Output dimension (model width)
    - K: Inner dimension (same for activations and weights)

    Fused operation:
    For each output element [m, n]:
      1. Dequantize weight column [k=0..K] on-the-fly from Q4_0 format
      2. Compute dot product: sum(activation[m, k] * dequantized_weight[k])
      3. Store to output[m, n]

    Kernel design:
    - Grid: (N, M, 1) - one block per output element
    - Block: 256 threads for dot product computation (wave-level reduction)
    - Shared memory: Cache dequantized weight column (K elements)
    - Each thread: Loads activation, dequantizes one weight, multiplies, reduces

    Follow patterns from qkt_matmul.hip for matmul structure.
    Follow patterns from q4_0_dequant.hip for Q4_0 unpacking.
  </action>
  <verify>q4_0_matmul.hip created with valid fused kernel</verify>
  <done>Q4_0 fused dequant-matmul HIP kernel created</done>
</task>

<task type="auto">
  <name>Task 2: Add fused kernel to build system</name>
  <files>build.rs</files>
  <action>
    Add Q4_0 matmul kernel to build.rs kernels array:

    ```rust
    (
        "kernels/q4_0_matmul.hip",
        "Q4_0_MATMUL_HSACO",
        "q4_0_matmul_kernel",
    ),
    ```
  </action>
  <verify>build.rs updated with q4_0_matmul entry</verify>
  <done>Fused kernel added to build system</done>
</task>

<task type="auto">
  <name>Task 3: Update quantized_matmul.rs with GPU implementation</name>
  <files>src/ggml/hip_backend/ops/quantized_matmul.rs</files>
  <action>
    Update src/ggml/hip_backend/ops/quantized_matmul.rs to use fused kernel:

    1. Add external function declaration:
       ```rust
       extern "C" {
           fn q4_0_matmul_kernel(
               activations: *const f32,
               weights_q4_0: *const u8,
               output: *mut f32,
               m: i32,
               n: i32,
               k: i32,
           );
       }
       ```

    2. Update matmul_q4_0 to use fused kernel instead of CPU dequant + GPU matmul:
       - Remove CPU dequantization step
       - Call fused kernel directly
       - Eliminates intermediate weight buffer allocation
       - Significant memory bandwidth savings

    3. Keep CPU fallback for when GPU unavailable or for small matrices where overhead isn't worth it
  </action>
  <verify>quantized_matmul.rs updated with fused kernel path</verify>
  <done>Q4_0 matmul updated to use fused GPU kernel</done>
</task>

<task type="auto">
  <name>Task 4: Add test for fused kernel</name>
  <files>src/ggml/hip_backend/ops/quantized_matmul.rs</files>
  <action>
    Add test validating fused kernel produces same result as dequant + matmul:

    Test approach:
    1. Create test activations (FP32) and Q4_0 weights
    2. Run fused kernel
    3. Run CPU dequant + GPU matmul (reference)
    4. Compare results (allow small floating-point tolerance)

    Test should be marked #[ignore] if it requires GPU.
  </action>
  <verify>Test created and compiles</verify>
  <done>Fused kernel test added</done>
</task>

</tasks>

<verification>
- [ ] kernels/q4_0_matmul.hip created with valid fused kernel
- [ ] build.rs updated
- [ ] quantized_matmul.rs updated to use fused kernel
- [ ] Test validates correctness vs reference implementation
- [ ] cargo check passes
</verification>

<success_criteria>
- Fused kernel compiles successfully
- Eliminates intermediate FP32 weight buffer allocation
- Produces same results as reference implementation (within tolerance)
- Memory bandwidth reduction for quantized inference
</success_criteria>

<output>
After completion, create `.planning/phases/05-quantized-operations/05-04-SUMMARY.md`
</output>
