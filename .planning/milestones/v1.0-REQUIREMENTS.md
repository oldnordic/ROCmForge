# Requirements Archive: v1.0 Production-Ready AMD GPU LLM Inference Engine

**Archived:** 2026-01-19
**Status:** ✅ SHIPPED

This is the archived requirements specification for v1.0.
For current requirements, see `.planning/REQUIREMENTS.md` (created for next milestone).

---

# ROCmForge Requirements (v1.0)

## What This Is

A high-performance LLM inference engine for AMD GPUs, written in Rust. ROCmForge loads GGUF-format models and runs them on ROCm/HIP with optimized CPU fallback using SIMD. It provides an OpenAI-compatible HTTP API for seamless integration with existing LLM applications.

## Core Value

**Reliable, fast inference on AMD GPUs with transparent CPU fallback.**

If ROCm is available, use it. If not, fall back to optimized CPU execution seamlessly. Any GGUF model should just work.

## Requirements

### Validated

*Capabilities shipped in v1.0:*

- [x] Fix inference hangs (GPU stream synchronization bug) — v1.0 (Phase 01-01)
- [x] Complete quantized matmul with native HIP dequantization kernel — v1.0 (Phase 05)
- [x] Implement flash attention detection and GPU kernels — v1.0 (Phase 06)
- [x] Add CPU SIMD backend for all tensor operations — v1.0 (Phases 4 + 12.1A)
- [x] Hybrid execution scheduler (automatic CPU/GPU op selection) — v1.0 (Phase 07)
- [x] Universal GGUF compatibility (all architectures, quantizations) — v1.0 (Phase 08)
- [x] Performance optimization (balanced: throughput, latency, memory) — v1.0 (Phase 09)
- [x] Production-ready reliability and error handling — v1.0 (Phase 10)

### Active

*Carry over to next milestone:*

(None remaining - all v1.0 requirements satisfied)

### Out of Scope

- **Training features** (LoRA adapters, fine-tuning, training modes) — Focus is inference-only
- **Non-text modalities** (vision, audio, multimodal models) — Text-only for v1
- **Multi-GPU/distributed execution** — Single GPU focus for v1
- **Non-AMD GPU support** — ROCm/HIP only (CPU fallback covers non-GPU systems)

## Context

**Shipped Codebase State (v1.0):**
- Modular Rust application with layered architecture (API → Service → Engine → Data → Kernel)
- 620 tests passing with unit + integration + E2E coverage
- ~64,900 lines of Rust code
- Complete error handling with proper Result types
- Tracing integration with OpenTelemetry support
- HTTP endpoints: /health, /ready, /metrics, /traces
- Comprehensive documentation: user guide, CLI reference, API docs, deployment guide

**Technical Environment:**
- Rust 2021 edition with Tokio async runtime
- ROCm/HIP for AMD GPU (targeting gfx1100 / RX 7900 XT, broader support achieved)
- Axum web framework for HTTP API
- GGUF format for model weights (llama.cpp compatibility)

**Known Issues (All Resolved in v1.0):**
- ✅ GPU stream synchronization bug — FIXED in Phase 01-01
- ✅ Race condition in inference loop — FIXED in Phase 01-02
- ✅ Engine cleanup issues in CLI — FIXED in Phase 01-03
- ✅ Missing .env.example — FIXED in Phase 10-12
- ✅ Test compilation errors — FIXED in Phase 11

## Constraints

- **Platform**: ROCm on Linux only — AMD GPU driver limitation
- **Model Format**: GGUF only — Leverage llama.cpp ecosystem
- **Hardware**: AMD GPU or CPU SIMD — No NVIDIA, no other accelerators
- **Architecture**: Single binary, self-contained — No external runtime dependencies beyond ROCm

## Key Decisions

| Decision | Rationale | Outcome |
|----------|-----------|---------|
| Rust implementation | Performance, safety, GPU FFI control | ✅ Successful |
| GGUF format only | llama.cpp ecosystem compatibility | ✅ 15 formats supported |
| Hybrid CPU/GPU execution | Maximum compatibility, graceful degradation | ✅ Working |
| OpenAI-compatible API | Drop-in replacement for existing apps | ✅ Implemented |
| Modular architecture with trait backends | Easy CPU/GPU switching, testability | ✅ Achieved |
| AVX-512 opt-in feature flag | Avoid CPU throttling on older hardware | ✅ Implemented |
| SQLiteGraph as optional dependency | Keep core lean while enabling context features | ✅ Feature-gated |

## Traceability

| Requirement | Phase | Status | Evidence |
|-------------|-------|--------|----------|
| Fix inference hangs | 1 | ✅ Complete | src/ggml/hip_backend/ops/matmul.rs:30-46 |
| Quantized matmul with HIP dequantization | 5 | ✅ Complete | kernels/q4_0_matmul.hip |
| Flash attention detection and GPU kernels | 6 | ✅ Complete | src/attention/flash_attention.rs |
| CPU SIMD backend for tensor operations | 4, 12.1A | ✅ Complete | src/attention/cpu.rs, src/backend/cpu/simd_ops.rs |
| Hybrid execution scheduler | 7 | ✅ Complete | src/ggml/hybrid_scheduler.rs |
| Universal GGUF compatibility | 8 | ✅ Complete | 15/15 quantization formats |
| Performance optimization | 9 | ✅ Complete | TTFT profiling, baseline storage |
| Production-ready reliability | 10 | ✅ Complete | Error handling, metrics, graceful degradation |

## Milestone Summary

**Shipped:** 8 of 8 v1.0 requirements (100%)

**Adjusted during implementation:**
- Phase 12 split into 12.1A (CPU SIMD) and 12.1B (Context Engine) to keep ROCmForge lean
- AVX-512 made opt-in via feature flag to avoid CPU throttling concerns
- HTTP endpoint for context engine skipped due to HnswIndex not being Send+Sync

**Dropped:** None

**Added (Post-Planning):**
- Phase 12.1A: CPU SIMD Completion (AVX-512 runtime detection, RMSNorm, RoPE, activations)
- Phase 12.1B: Context Engine Integration (SQLiteGraph-based semantic context)

---

*Archived: 2026-01-19 as part of v1.0 milestone completion*
