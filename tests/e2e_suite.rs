//! Comprehensive End-to-End Test Suite for ROCmForge
//!
//! This test suite provides complete E2E validation of the entire inference pipeline,
//! from low-level GPU loading through high-level token generation. These tests use
//! real GGUF models and validate the complete system behavior.
//!
//! # Test Coverage
//!
//! ## Part 1: Low-Level Async GPU Loading (6 tests)
//! 1. **Basic Async Loading** - Verify `load_to_gpu_async()` actually works
//! 2. **Performance Comparison** - Measure actual speedup vs sequential loading
//! 3. **Correctness Validation** - Verify loaded data matches byte-for-byte
//! 4. **Concurrent Stress Test** - Verify thread safety with multiple loaders
//! 5. **Cache Behavior** - Document cache hit/miss patterns
//! 6. **Memory Safety** - Verify no memory leaks
//!
//! ## Part 2: High-Level Inference Pipeline (6 tests)
//! 7. **Model Loading E2E** - Load real GGUF models and validate structure
//! 8. **Inference Execution E2E** - Run actual inference and validate token generation
//! 9. **KV Cache E2E** - Verify KV cache works correctly across multiple tokens
//! 10. **Scheduler E2E** - Test request queuing, batching, and scheduling
//! 11. **Error Recovery E2E** - Validate graceful failure on invalid inputs
//! 12. **Full Pipeline E2E** - Complete end-to-end validation
//!
//! # Requirements
//!
//! - ROCm/HIP runtime (AMD GPU)
//! - Small GGUF model (e.g., qwen2.5-0.5b.gguf or bge-small-en-v1.5.Q8_0.gguf)
//! - ~5GB GPU memory for concurrent loading test
//! - Tests skip gracefully if models are unavailable
//!
//! # Test Execution
//!
//! ```bash
//! # Run all E2E tests (serial execution for GPU safety)
//! cargo test --test e2e_suite --features rocm -- --test-threads=1
//!
//! # Run specific test
//! cargo test --test e2e_suite test_async_loading_basic --features rocm -- --test-threads=1
//!
//! # Run including ignored tests (slow performance/stress tests)
//! cargo test --test e2e_suite --features rocm -- --ignored --test-threads=1
//! ```

// =============================================================================
// PART 1: Low-Level Async GPU Loading Tests
// =============================================================================

use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;

use rocmforge::backend::hip_backend::{DeviceTensor, HipBackend};

// -----------------------------------------------------------------------------
// GPU Test Fixture (for integration tests in tests/ directory)
// NOTE: Integration tests cannot import from tests/common, so we duplicate
// the fixture here. This is identical to tests/common/mod.rs::GpuTestFixture.
// -----------------------------------------------------------------------------

/// GPU test fixture for E2E tests
///
/// This fixture provides shared GPU backend across all tests with:
/// - GPU availability checking
/// - Memory leak detection
/// - Conservative memory allocation (70% of free)
///
/// NOTE: This is duplicated from tests/common/mod.rs because integration tests
/// in the tests/ directory are compiled as separate crates and cannot import
/// from tests/common directly.
struct GpuFixture {
    backend: Arc<HipBackend>,
    initial_free_mb: usize,
    initial_total_mb: usize,
    device_name: String,
}

impl GpuFixture {
    fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let backend = HipBackend::new_checked()?;
        let (free, total) = backend.get_memory_info()?;
        let device_name = backend.device().name.clone();

        Ok(Self {
            backend,
            initial_free_mb: free / 1024 / 1024,
            initial_total_mb: total / 1024 / 1024,
            device_name,
        })
    }

    fn backend(&self) -> &Arc<HipBackend> {
        &self.backend
    }

    fn device_name(&self) -> &str {
        &self.device_name
    }

    fn total_memory_mb(&self) -> usize {
        self.initial_total_mb
    }

    fn free_memory_mb(&self) -> usize {
        self.initial_free_mb
    }

    fn safe_alloc_mb(&self) -> usize {
        (self.initial_free_mb * 7) / 10
    }

    fn assert_no_leak(&self, tolerance_percent: usize) {
        let (free, _total) = self
            .backend
            .get_memory_info()
            .expect("Failed to query GPU memory");

        let free_mb = free / 1024 / 1024;
        let leaked_mb = self.initial_free_mb.saturating_sub(free_mb);
        let tolerance_mb = (self.initial_total_mb * tolerance_percent) / 100;

        if leaked_mb > tolerance_mb {
            panic!(
                "ðŸš¨ GPU memory leak detected!\n\
                 Initial free: {} MB\n\
                 Current free: {} MB\n\
                 Leaked: {} MB\n\
                 Tolerance: {} MB ({}%)\n\
                 ðŸ’¡ Tip: Make sure DeviceTensors are dropped before end of test",
                self.initial_free_mb, free_mb, leaked_mb, tolerance_mb, tolerance_percent
            );
        }
    }

    fn memory_stats(&self) -> (usize, usize) {
        match self.backend.get_memory_info() {
            Ok((free, total)) => (free / 1024 / 1024, total / 1024 / 1024),
            Err(_) => (0, 0),
        }
    }
}

/// Global GPU fixture for E2E tests
///
/// NOTE: This is duplicated from tests/common/mod.rs because integration tests
/// cannot import from it directly.
static GPU_FIXTURE: once_cell::sync::Lazy<Option<GpuFixture>> = once_cell::sync::Lazy::new(|| {
    if !HipBackend::gpu_available() {
        eprintln!("âš ï¸  WARNING: GPU not available - skipping GPU tests");
        eprintln!("To enable GPU tests, ensure:");
        eprintln!("  1. AMD GPU is present");
        eprintln!("  2. ROCm is installed (check with rocm-smi)");
        eprintln!("  3. amdhip64 library is in LD_LIBRARY_PATH");
        return None;
    }

    match GpuFixture::new() {
        Ok(fixture) => {
            eprintln!("âœ… GPU Test Fixture initialized");
            eprintln!("   Device: {}", fixture.device_name());
            eprintln!("   Total Memory: {} MB", fixture.total_memory_mb());
            eprintln!("   Free Memory: {} MB", fixture.free_memory_mb());
            eprintln!("   Safe Alloc Limit: {} MB", fixture.safe_alloc_mb());
            Some(fixture)
        }
        Err(e) => {
            eprintln!("âŒ ERROR: Failed to initialize GPU test fixture: {}", e);
            eprintln!("   GPU tests will be skipped");
            None
        }
    }
});

// -----------------------------------------------------------------------------
// Test Configuration and Helpers
// -----------------------------------------------------------------------------

/// Convert PathBuf to &str, returning None if invalid
fn path_to_str(path: &PathBuf) -> Option<&str> {
    path.to_str()
}

/// Portable model path resolution
///
/// Searches for test models in the following order:
/// 1. `ROCmForge/models/` relative to CARGO_MANIFEST_DIR
/// 2. `models/` relative to current working directory
/// 3. Environment variable `ROCmFORGE_MODEL_DIR`
fn get_test_model_path() -> Option<PathBuf> {
    // Try CARGO_MANIFEST_DIR/models first
    if let Ok(manifest_dir) = std::env::var("CARGO_MANIFEST_DIR") {
        let models_dir = PathBuf::from(manifest_dir).join("models");
        for model in &["qwen2.5-0.5b.gguf", "bge-small-en-v1.5.Q8_0.gguf"] {
            let path = models_dir.join(model);
            if path.exists() {
                return Some(path);
            }
        }
    }

    // Try current directory models/
    let models_dir = PathBuf::from("models");
    for model in &["qwen2.5-0.5b.gguf", "bge-small-en-v1.5.Q8_0.gguf"] {
        let path = models_dir.join(model);
        if path.exists() {
            return Some(path);
        }
    }

    // Try environment variable
    if let Ok(model_dir) = std::env::var("ROCmFORGE_MODEL_DIR") {
        for model in &["qwen2.5-0.5b.gguf", "bge-small-en-v1.5.Q8_0.gguf"] {
            let path = PathBuf::from(&model_dir).join(model);
            if path.exists() {
                return Some(path);
            }
        }
    }

    None
}

/// Helper: Calculate mean of a slice
fn mean(values: &[f32]) -> f32 {
    if values.is_empty() {
        return 0.0;
    }
    values.iter().sum::<f32>() / values.len() as f32
}

/// Helper: Calculate standard deviation of a slice
fn std_dev(values: &[f32]) -> f32 {
    if values.len() < 2 {
        return 0.0;
    }
    let avg = mean(values);
    let variance =
        values.iter().map(|&x| (x - avg).powi(2)).sum::<f32>() / (values.len() - 1) as f32;
    variance.sqrt()
}

/// Helper: Verify two tensors have matching data (within floating point tolerance)
fn tensors_match(t1: &DeviceTensor, t2: &DeviceTensor, tolerance: f32) -> anyhow::Result<bool> {
    if t1.shape().dims() != t2.shape().dims() {
        return Ok(false);
    }

    let len = t1.len();
    if len != t2.len() {
        return Ok(false);
    }

    // Copy both to host for comparison
    let data1 = t1.to_host_vec()?;
    let data2 = t2.to_host_vec()?;

    // Check element-wise equality with tolerance
    let max_diff = data1
        .iter()
        .zip(data2.iter())
        .map(|(a, b)| (a - b).abs())
        .fold(0.0f32, f32::max);

    Ok(max_diff <= tolerance)
}

/// Helper: Check if two HashMaps have the same tensor keys
fn same_tensor_keys(
    map1: &HashMap<String, DeviceTensor>,
    map2: &HashMap<String, DeviceTensor>,
) -> bool {
    let keys1: std::collections::HashSet<_> = map1.keys().collect();
    let keys2: std::collections::HashSet<_> = map2.keys().collect();
    keys1 == keys2
}

// -----------------------------------------------------------------------------
// Test 1: Basic Async Loading
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
fn test_async_loading_basic() {
    // Skip test if no GPU available
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    // Skip test if no model available
    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            println!("  Searched in: CARGO_MANIFEST_DIR/models, ./models, $ROCmFORGE_MODEL_DIR");
            println!("  Expected: qwen2.5-0.5b.gguf or bge-small-en-v1.5.Q8_0.gguf");
            return;
        }
    };

    println!("\n=== Test 1: Basic Async Loading ===");
    println!("Model: {}", model_path.display());
    println!("GPU: {}", fixture.device_name());

    // Load GGUF model
    let model_path_str = match model_path.to_str() {
        Some(s) => s,
        None => {
            println!("SKIP: Invalid model path (non-UTF8)");
            return;
        }
    };
    let loader = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            println!("SKIP: Failed to load GGUF: {}", e);
            return;
        }
    };

    println!("Metadata: {}", loader.metadata().architecture);
    println!("Layers: {}", loader.metadata().num_layers);
    println!("Hidden size: {}", loader.metadata().hidden_size);

    // Load tensors using async method
    let start = Instant::now();
    let tensors_async = match loader.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Failed to load tensors with async method: {}", e);
        }
    };
    let async_duration = start.elapsed();

    println!("Async loading time: {:?}", async_duration);
    println!("Loaded {} tensors", tensors_async.len());

    // Verify we loaded some tensors
    assert!(!tensors_async.is_empty(), "Should load at least one tensor");
    assert!(
        tensors_async.len() >= 100,
        "Real model should have at least 100 tensors, got {}",
        tensors_async.len()
    );

    // Verify tensor shapes are valid
    for (name, tensor) in tensors_async.iter().take(5) {
        let shape = tensor.shape().dims();
        println!("  {}: {:?} ({} elements)", name, shape, tensor.len());
        assert!(
            !shape.is_empty(),
            "Tensor {} should have non-empty shape",
            name
        );
        assert!(tensor.len() > 0, "Tensor {} should have > 0 elements", name);
    }

    // Check for memory leaks before test ends
    fixture.assert_no_leak(5);

    println!("âœ“ Basic async loading test passed\n");
}

// -----------------------------------------------------------------------------
// Test 2: Performance Comparison
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
#[ignore = "Performance test - slow, run with: cargo test --test e2e_suite --ignored -- --test-threads=1"]
fn test_async_loading_performance() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 2: Performance Comparison ===");
    println!("Model: {}", model_path.display());

    // Load GGUF model (create fresh loader for each test)
    let model_path_str = match path_to_str(&model_path) {
        Some(s) => s,
        None => {
            println!("SKIP: Invalid model path");
            return;
        }
    };
    let loader1 = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            println!("SKIP: Failed to load GGUF: {}", e);
            return;
        }
    };

    // Benchmark sequential loading (old method)
    print!("Loading sequentially (old method)... ");
    let start = Instant::now();
    let tensors_seq = match loader1.load_to_gpu(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Sequential loading failed: {}", e);
        }
    };
    let seq_duration = start.elapsed();
    println!("{:?}", seq_duration);

    // Create fresh loader for async test
    let loader2 = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            panic!("Failed to reload GGUF: {}", e);
        }
    };

    // Benchmark async loading (new method)
    print!("Loading asynchronously (new method)... ");
    let start = Instant::now();
    let tensors_async = match loader2.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Async loading failed: {}", e);
        }
    };
    let async_duration = start.elapsed();
    println!("{:?}", async_duration);

    // Calculate speedup
    let speedup = seq_duration.as_secs_f64() / async_duration.as_secs_f64();

    println!("\nResults:");
    println!(
        "  Sequential: {:?} ({} tensors)",
        seq_duration,
        tensors_seq.len()
    );
    println!(
        "  Async:      {:?} ({} tensors)",
        async_duration,
        tensors_async.len()
    );
    println!("  Speedup:    {:.2}x", speedup);

    // Assertions
    assert_eq!(
        tensors_seq.len(),
        tensors_async.len(),
        "Should load same number of tensors"
    );

    // Assert at least 2x speedup (conservative threshold)
    // Real-world is ~5x, but we use 2x to account for variance
    assert!(
        speedup >= 2.0,
        "Async loading should be at least 2x faster, got {:.2}x",
        speedup
    );

    // Clean up tensors before leak check
    drop(tensors_seq);
    drop(tensors_async);

    fixture.assert_no_leak(5);

    println!("\nâœ“ Performance test passed: {:.2}x speedup", speedup);
}

// -----------------------------------------------------------------------------
// Test 3: Correctness Validation
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
fn test_async_loading_correctness() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 3: Correctness Validation ===");
    println!("Model: {}", model_path.display());

    // Load with sequential method
    let model_path_str = match path_to_str(&model_path) {
        Some(s) => s,
        None => {
            println!("SKIP: Invalid model path");
            return;
        }
    };
    let loader1 = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            println!("SKIP: Failed to load GGUF: {}", e);
            return;
        }
    };

    let tensors_seq = match loader1.load_to_gpu(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Sequential loading failed: {}", e);
        }
    };

    // Load with async method
    let loader2 = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            panic!("Failed to reload GGUF: {}", e);
        }
    };

    let tensors_async = match loader2.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Async loading failed: {}", e);
        }
    };

    println!("Loaded {} tensors (sequential)", tensors_seq.len());
    println!("Loaded {} tensors (async)", tensors_async.len());

    // Verify same tensor names
    assert!(
        same_tensor_keys(&tensors_seq, &tensors_async),
        "Should have same tensor names"
    );

    // Spot-check first few tensors for data correctness
    let tolerance = 1e-6; // Floating point tolerance
    let mut checked = 0;
    let mut errors = 0;

    for name in tensors_seq.keys().take(10) {
        let t_seq = &tensors_seq[name];
        let t_async = match tensors_async.get(name) {
            Some(t) => t,
            None => {
                println!("  ERROR: Async missing tensor: {}", name);
                errors += 1;
                continue;
            }
        };

        match tensors_match(t_seq, t_async, tolerance) {
            Ok(true) => {
                println!("  âœ“ {} matches ({} elements)", name, t_seq.len());
                checked += 1;
            }
            Ok(false) => {
                println!("  âœ— {} differs ({} elements)", name, t_seq.len());
                errors += 1;
            }
            Err(e) => {
                println!("  âœ— {} error: {}", name, e);
                errors += 1;
            }
        }
    }

    println!("\nChecked {} tensors", checked);
    if errors > 0 {
        println!("ERRORS: {}", errors);
    }

    assert_eq!(errors, 0, "All checked tensors should match");
    assert!(checked >= 5, "Should verify at least 5 tensors");

    // Clean up before leak check
    drop(tensors_seq);
    drop(tensors_async);

    fixture.assert_no_leak(5);

    println!("âœ“ Correctness validation passed\n");
}

// -----------------------------------------------------------------------------
// Test 4: Concurrent Loading Stress Test
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
#[ignore = "Stress test - requires ~5GB GPU memory"]
fn test_async_loading_concurrent() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 4: Concurrent Loading Stress Test ===");
    println!("Model: {}", model_path.display());

    // Check available memory
    let (free_mb, _) = fixture.memory_stats();
    println!("GPU memory: {} MB free", free_mb);

    if free_mb < 5 * 1024 {
        println!("SKIP: Need at least 5GB free GPU memory for concurrent test");
        return;
    }

    use std::sync::Arc;
    use std::thread;

    // Create multiple loaders in parallel
    let num_threads = 3;
    let backend_ref = fixture.backend().clone();

    println!("Spawning {} concurrent loaders...", num_threads);

    let mut handles = Vec::new();

    for thread_id in 0..num_threads {
        let backend_clone = Arc::clone(&backend_ref);
        let model_path_clone = model_path.clone();

        let handle = thread::spawn(move || {
            println!("  Thread {} starting...", thread_id);

            let start = Instant::now();

            // Create loader and load tensors
            let model_path_str = match path_to_str(&model_path_clone) {
                Some(s) => s,
                None => {
                    return Ok((thread_id, Duration::from_secs(0)));
                }
            };

            let loader = match GgufLoader::new(model_path_str) {
                Ok(l) => l,
                Err(e) => {
                    return Err(format!("Thread {}: Failed to load GGUF: {}", thread_id, e));
                }
            };

            let tensors = match loader.load_to_gpu_async(&backend_clone) {
                Ok(t) => t,
                Err(e) => {
                    return Err(format!("Thread {}: Failed to load async: {}", thread_id, e));
                }
            };

            let duration = start.elapsed();

            println!(
                "  Thread {} completed: {:?} ({} tensors)",
                thread_id,
                duration,
                tensors.len()
            );

            // Clean up tensors before returning
            drop(tensors);

            Ok((thread_id, duration))
        });

        handles.push(handle);
    }

    // Wait for all threads and collect results
    let mut results = Vec::new();
    for handle in handles {
        match handle.join() {
            Ok(Ok(result)) => results.push(result),
            Ok(Err(e)) => panic!("Thread failed: {}", e),
            Err(e) => panic!("Thread panicked: {:?}", e),
        }
    }

    println!("\nConcurrent loading results:");
    for (thread_id, duration) in results.iter() {
        println!("  Thread {}: {:?}", thread_id, duration);
    }

    // Verify all threads loaded successfully
    assert_eq!(results.len(), num_threads, "All threads should complete");

    fixture.assert_no_leak(10); // Allow 10% tolerance for concurrent test

    println!("\nâœ“ Concurrent stress test passed\n");
}

// -----------------------------------------------------------------------------
// Test 5: Cache Behavior
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
fn test_async_loading_cache_behavior() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 5: Cache Behavior ===");
    println!("Model: {}", model_path.display());

    let model_path_str = match path_to_str(&model_path) {
        Some(s) => s,
        None => {
            println!("SKIP: Invalid model path");
            return;
        }
    };
    let loader = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            println!("SKIP: Failed to load GGUF: {}", e);
            return;
        }
    };

    // First load (should hit disk)
    print!("First load (cold cache)... ");
    let start = Instant::now();
    let tensors1 = match loader.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("First load failed: {}", e);
        }
    };
    let duration1 = start.elapsed();
    println!("{:?}", duration1);

    // Clean up first load
    drop(tensors1);

    // Second load (should hit cache if implemented, otherwise same)
    // Note: Current implementation bypasses cache, so this should be same time
    print!("Second load (warm cache)... ");
    let start = Instant::now();
    let tensors2 = match loader.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Second load failed: {}", e);
        }
    };
    let duration2 = start.elapsed();
    println!("{:?}", duration2);

    // Clean up second load
    drop(tensors2);

    println!("\nCache behavior:");
    println!("  First load:  {:?}", duration1);
    println!("  Second load: {:?}", duration2);

    // Current implementation: async mode bypasses cache
    // So both should take similar time (within 3x tolerance for variance)
    let ratio = duration2.as_secs_f64() / duration1.as_secs_f64();
    println!("  Ratio: {:.2}x", ratio);

    // This documents current behavior (not a strict assertion)
    if ratio < 0.5 {
        println!("  NOTE: Second load was much faster - cache may be working");
    } else {
        println!("  NOTE: Async mode bypasses cache (expected)");
    }

    fixture.assert_no_leak(5);

    println!("âœ“ Cache behavior test completed\n");
}

// -----------------------------------------------------------------------------
// Test 6: Memory Safety
// -----------------------------------------------------------------------------

#[test]
#[cfg(feature = "rocm")]
#[serial]
fn test_async_loading_memory_safety() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_test_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 6: Memory Safety ===");
    println!("Model: {}", model_path.display());

    let (free_before, total_before) = fixture.memory_stats();
    println!(
        "Initial GPU memory: {} MB free / {} MB total",
        free_before, total_before
    );

    // Load tensors
    let model_path_str = match path_to_str(&model_path) {
        Some(s) => s,
        None => {
            println!("SKIP: Invalid model path");
            return;
        }
    };
    let loader = match GgufLoader::new(model_path_str) {
        Ok(l) => l,
        Err(e) => {
            println!("SKIP: Failed to load GGUF: {}", e);
            return;
        }
    };

    let tensors = match loader.load_to_gpu_async(fixture.backend()) {
        Ok(t) => t,
        Err(e) => {
            panic!("Async loading failed: {}", e);
        }
    };

    println!("Loaded {} tensors", tensors.len());

    let (free_after, total_after) = fixture.memory_stats();
    println!(
        "After load: {} MB free / {} MB total",
        free_after, total_after
    );

    let used = free_before.saturating_sub(free_after);
    println!("GPU memory used: ~{} MB", used);

    // Verify memory usage is reasonable
    // Small models should use < 4GB
    assert!(
        used < 4096,
        "Small model should use < 4GB, used ~{} MB",
        used
    );

    // Drop tensors and verify memory is freed
    drop(tensors);
    println!("Tensors dropped");

    let (free_final, _) = fixture.memory_stats();

    if free_final > 0 && free_before > 0 {
        let recovered = free_final.saturating_sub(free_before);
        println!("Memory recovered after drop: ~{} MB", recovered);

        // Most memory should be recovered
        // Allow 10% tolerance for fragmentation
        let expected_recovered = free_before.saturating_sub(free_after);
        assert!(
            recovered >= (expected_recovered * 9 / 10) || recovered == 0,
            "Should recover most memory after dropping tensors"
        );
    }

    // Final leak check with tolerance
    fixture.assert_no_leak(5);

    println!("âœ“ Memory safety test passed\n");
}

// =============================================================================
// PART 2: High-Level Inference Pipeline Tests
// =============================================================================

use std::time::Duration;

use rocmforge::engine::{EngineConfig, InferenceEngine};
use rocmforge::tokenizer::TokenizerAdapter;

// -----------------------------------------------------------------------------
// Test Configuration and Helpers
// -----------------------------------------------------------------------------

/// Get the first available model path for inference tests
fn get_inference_model_path() -> Option<PathBuf> {
    get_test_model_path()
}

/// Check if GPU is available
fn gpu_available() -> bool {
    #[cfg(feature = "rocm")]
    {
        rocmforge::backend::HipBackend::gpu_available()
    }
    #[cfg(not(feature = "rocm"))]
    {
        false
    }
}

/// Helper: Create and initialize an engine with the given model
async fn create_engine_with_model(model_path: &PathBuf) -> anyhow::Result<Arc<InferenceEngine>> {
    let mut engine = InferenceEngine::new(EngineConfig::default())?;
    engine.load_gguf_model(model_path).await?;
    let engine = Arc::new(engine);
    engine.start().await?;

    // Spawn inference loop in background
    let engine_clone = engine.clone();
    tokio::spawn(async move {
        let _ = engine_clone.run_inference_loop().await;
    });

    // Give inference loop time to start
    tokio::time::sleep(Duration::from_millis(100)).await;

    Ok(engine)
}

/// Helper: Get or infer tokenizer path
fn get_tokenizer(model_path: &PathBuf) -> TokenizerAdapter {
    // Try to infer tokenizer path from model path
    let model_dir = model_path
        .parent()
        .unwrap_or_else(|| std::path::Path::new("."));
    let inferred_tokenizer = model_dir.join("tokenizer.json");

    let tokenizer_path = if inferred_tokenizer.exists() {
        Some(inferred_tokenizer.to_str().unwrap().to_string())
    } else {
        None
    };

    // Try embedded tokenizer from GGUF
    let embedded = model_path
        .to_str()
        .and_then(|p| rocmforge::tokenizer::embedded_tokenizer_from_gguf(p));

    TokenizerAdapter::from_spec(
        tokenizer_path.as_deref(),
        embedded.as_ref().map(|t| t.json.as_str()),
    )
}

// -----------------------------------------------------------------------------
// Test 7: Model Loading E2E
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
async fn test_model_loading_e2e() {
    // Skip test if no GPU available
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 7: Model Loading E2E ===");
    println!("Model: {}", model_path.display());
    println!("GPU: {}", fixture.device_name());

    let start = Instant::now();

    // Create engine and load model
    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    let load_time = start.elapsed();
    println!("Model loaded in {:?}", load_time);

    // Verify engine stats
    let stats = engine.get_engine_stats().await;
    assert!(stats.is_running, "Engine should be running");
    assert!(stats.model_loaded, "Model should be loaded");

    println!(
        "Engine stats: running={}, model_loaded={}",
        stats.is_running, stats.model_loaded
    );

    // Verify scheduler stats
    assert_eq!(
        stats.scheduler_stats.pending_requests, 0,
        "Should have no pending requests"
    );
    assert_eq!(
        stats.scheduler_stats.processing_requests, 0,
        "Should have no processing requests"
    );

    // Verify KV cache stats
    assert_eq!(
        stats.cache_stats.active_sequences, 0,
        "Should have no active sequences"
    );

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("âœ“ Model loading E2E test passed\n");
}

// -----------------------------------------------------------------------------
// Test 8: Inference Execution E2E
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
async fn test_inference_execution_e2e() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 8: Inference Execution E2E ===");
    println!("Model: {}", model_path.display());

    // Create engine with model
    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    // Get tokenizer
    let tokenizer = get_tokenizer(&model_path);
    let prompt = "The quick brown fox";
    let prompt_tokens = tokenizer.encode(prompt);

    println!("Prompt: \"{}\"", prompt);
    println!(
        "Prompt tokens: {:?} ({} tokens)",
        prompt_tokens,
        prompt_tokens.len()
    );

    // Submit inference request
    let max_tokens = 10;
    let temperature = 0.8;
    let top_k = 50;
    let top_p = 0.9;

    let request_id = match engine
        .submit_request(prompt_tokens.clone(), max_tokens, temperature, top_k, top_p)
        .await
    {
        Ok(id) => id,
        Err(e) => {
            println!("SKIP: Failed to submit request: {}", e);
            engine.stop().await.ok();
            return;
        }
    };

    println!("Request ID: {}", request_id);

    // Wait for completion with timeout
    let start = Instant::now();
    let timeout = Duration::from_secs(30);

    loop {
        let status = match engine.get_request_status(request_id).await {
            Ok(Some(s)) => s,
            Ok(None) => {
                println!("ERROR: Request {} disappeared", request_id);
                engine.stop().await.ok();
                panic!("Request disappeared during inference");
            }
            Err(e) => {
                println!("ERROR: Failed to get request status: {}", e);
                engine.stop().await.ok();
                panic!("Failed to get request status: {}", e);
            }
        };

        if status.is_complete() {
            let generated_count = status.generated_tokens.len();
            let text = tokenizer.decode(&status.generated_tokens);

            println!("Inference completed in {:?}", start.elapsed());
            println!("Generated {} tokens", generated_count);
            println!("Generated text: \"{}\"", text);
            println!("Finish reason: {:?}", status.finish_reason);

            // Verify we generated some tokens (at least 1, at most max_tokens)
            assert!(
                generated_count >= 1 && generated_count <= max_tokens,
                "Generated token count {} should be between 1 and {}",
                generated_count,
                max_tokens
            );

            // Verify finish reason
            assert!(
                status.finish_reason.is_some() || generated_count == max_tokens,
                "Should have finish reason or reached max_tokens"
            );

            break;
        }

        // Check timeout
        if start.elapsed() > timeout {
            println!("ERROR: Inference timed out after {:?}", timeout);
            engine.stop().await.ok();
            panic!("Inference timed out");
        }

        // Small sleep before polling again
        tokio::time::sleep(Duration::from_millis(50)).await;
    }

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("âœ“ Inference execution E2E test passed\n");
}

// -----------------------------------------------------------------------------
// Test 9: KV Cache E2E
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
async fn test_kv_cache_e2e() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 9: KV Cache E2E ===");
    println!("Model: {}", model_path.display());

    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    let tokenizer = get_tokenizer(&model_path);

    // Submit a request and verify KV cache is populated
    let prompt = "Hello world";
    let prompt_tokens = tokenizer.encode(prompt);

    let request_id = match engine
        .submit_request(prompt_tokens.clone(), 5, 0.8, 50, 0.9)
        .await
    {
        Ok(id) => id,
        Err(e) => {
            println!("SKIP: Failed to submit request: {}", e);
            engine.stop().await.ok();
            return;
        }
    };

    println!("Request ID: {}", request_id);

    // Wait for request to start processing
    let start = Instant::now();
    let timeout = Duration::from_secs(30);

    loop {
        let stats = engine.get_engine_stats().await;

        // Check if request is processing (KV cache should be active)
        if stats.cache_stats.active_sequences > 0 {
            println!(
                "KV cache active: {} sequences",
                stats.cache_stats.active_sequences
            );
            println!("KV cache tokens: {}", stats.cache_stats.total_tokens);

            // Verify cache is tracking our sequence
            assert!(
                stats.cache_stats.active_sequences >= 1,
                "Should have at least 1 active sequence"
            );

            break;
        }

        // Also check if request completed (cache might be cleared)
        let status = engine.get_request_status(request_id).await;
        if let Ok(Some(s)) = status {
            if s.is_complete() {
                println!("Request completed before cache check");
                break;
            }
        }

        if start.elapsed() > timeout {
            println!("WARNING: KV cache timeout - request may not have started");
            break;
        }

        tokio::time::sleep(Duration::from_millis(50)).await;
    }

    // Wait for completion
    loop {
        let status = match engine.get_request_status(request_id).await {
            Ok(Some(s)) => s,
            _ => break,
        };

        if status.is_complete() {
            println!("Request completed");
            break;
        }

        if start.elapsed() > timeout {
            println!("WARNING: Request timeout");
            break;
        }

        tokio::time::sleep(Duration::from_millis(50)).await;
    }

    // Verify cache stats after completion
    let final_stats = engine.get_engine_stats().await;
    println!(
        "Final cache stats: active_sequences={}, total_tokens={}",
        final_stats.cache_stats.active_sequences, final_stats.cache_stats.total_tokens
    );

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("âœ“ KV cache E2E test passed\n");
}

// -----------------------------------------------------------------------------
// Test 10: Scheduler E2E (Queuing and Batching)
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
async fn test_scheduler_e2e() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 10: Scheduler E2E ===");
    println!("Model: {}", model_path.display());

    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    let tokenizer = get_tokenizer(&model_path);

    // Submit multiple requests to test queuing
    let num_requests = 3;
    let mut request_ids = Vec::new();

    println!("Submitting {} requests...", num_requests);

    for i in 0..num_requests {
        let prompt = format!("Prompt {}", i);
        let prompt_tokens = tokenizer.encode(&prompt);

        match engine.submit_request(prompt_tokens, 3, 0.8, 50, 0.9).await {
            Ok(id) => {
                request_ids.push(id);
                println!("  Request {} submitted: ID {}", i, id);
            }
            Err(e) => {
                println!("  ERROR: Failed to submit request {}: {}", i, e);
                engine.stop().await.ok();
                return;
            }
        }
    }

    // Check scheduler stats
    let stats = engine.get_engine_stats().await;
    println!(
        "Scheduler stats: pending={}, processing={}, completed={}",
        stats.scheduler_stats.pending_requests,
        stats.scheduler_stats.processing_requests,
        stats.scheduler_stats.completed_requests
    );

    // Verify all requests are tracked
    let total_tracked = stats.scheduler_stats.pending_requests
        + stats.scheduler_stats.processing_requests
        + stats.scheduler_stats.completed_requests;

    assert!(
        total_tracked >= num_requests,
        "Should track at least {} requests, tracked {}",
        num_requests,
        total_tracked
    );

    // Wait for all requests to complete
    let start = Instant::now();
    let timeout = Duration::from_secs(60);

    loop {
        let stats = engine.get_engine_stats().await;
        let completed = stats.scheduler_stats.completed_requests;

        if completed >= num_requests {
            println!("All {} requests completed", completed);
            break;
        }

        if start.elapsed() > timeout {
            println!(
                "WARNING: Only {}/{} requests completed after {:?}",
                completed, num_requests, timeout
            );
            break;
        }

        tokio::time::sleep(Duration::from_millis(100)).await;
    }

    // Verify all requests completed successfully
    let mut success_count = 0;
    for request_id in request_ids {
        if let Ok(Some(status)) = engine.get_request_status(request_id).await {
            if status.is_complete() {
                success_count += 1;
            }
        }
    }

    println!(
        "Successfully completed: {}/{} requests",
        success_count, num_requests
    );

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("âœ“ Scheduler E2E test passed\n");
}

// -----------------------------------------------------------------------------
// Test 11: Error Recovery E2E
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
async fn test_error_recovery_e2e() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 11: Error Recovery E2E ===");
    println!("Model: {}", model_path.display());

    // Test 11.1: Invalid model path
    println!("\n11.1: Testing invalid model path...");
    let mut engine = match InferenceEngine::new(EngineConfig::default()) {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    let result = engine
        .load_gguf_model("/nonexistent/path/to/model.gguf")
        .await;
    assert!(result.is_err(), "Should fail to load nonexistent model");
    println!("  âœ“ Correctly rejected invalid path");

    // Test 11.2: Empty prompt
    println!("\n11.2: Testing empty prompt...");
    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };

    let result = engine.submit_request(vec![], 10, 0.8, 50, 0.9).await;
    // Empty prompt should either succeed (degenerate case) or fail gracefully
    match result {
        Ok(_) => println!("  âœ“ Empty prompt accepted (degenerate case)"),
        Err(e) => println!("  âœ“ Empty prompt rejected: {}", e),
    }

    // Test 11.3: Invalid sampling parameters
    println!("\n11.3: Testing invalid sampling parameters...");

    // Invalid temperature (negative)
    let result = engine
        .submit_request(vec![1, 2, 3], 10, -0.5, 50, 0.9)
        .await;
    assert!(result.is_err(), "Should reject negative temperature");
    println!("  âœ“ Rejected negative temperature");

    // Invalid top_p (out of range)
    let result = engine.submit_request(vec![1, 2, 3], 10, 0.8, 50, 1.5).await;
    assert!(result.is_err(), "Should reject top_p > 1.0");
    println!("  âœ“ Rejected top_p > 1.0");

    // Invalid max_tokens (zero)
    let result = engine.submit_request(vec![1, 2, 3], 0, 0.8, 50, 0.9).await;
    assert!(result.is_err(), "Should reject max_tokens = 0");
    println!("  âœ“ Rejected max_tokens = 0");

    // Test 11.4: Request cancellation
    println!("\n11.4: Testing request cancellation...");
    let tokenizer = get_tokenizer(&model_path);
    let prompt_tokens = tokenizer.encode("Cancel this request");

    let request_id = match engine.submit_request(prompt_tokens, 10, 0.8, 50, 0.9).await {
        Ok(id) => id,
        Err(e) => {
            println!(
                "WARNING: Failed to submit request for cancellation test: {}",
                e
            );
            engine.stop().await.ok();
            return;
        }
    };

    // Cancel immediately
    match engine.cancel_request(request_id).await {
        Ok(_) => println!("  âœ“ Request cancelled successfully"),
        Err(e) => println!("  WARNING: Cancellation failed: {}", e),
    }

    // Verify request was cancelled
    if let Ok(Some(status)) = engine.get_request_status(request_id).await {
        if status.state == rocmforge::scheduler::RequestState::Cancelled {
            println!("  âœ“ Request state is Cancelled");
        } else {
            println!(
                "  WARNING: Request state is {:?}, not Cancelled",
                status.state
            );
        }
    }

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("âœ“ Error recovery E2E test passed\n");
}

// -----------------------------------------------------------------------------
// Test 12: Full Pipeline E2E (Slow - Ignored by Default)
// -----------------------------------------------------------------------------

#[tokio::test]
#[cfg(feature = "rocm")]
#[serial]
#[ignore = "Full pipeline test - slow, run with: cargo test --test e2e_suite --ignored -- --test-threads=1"]
async fn test_full_pipeline_e2e() {
    let fixture = match GPU_FIXTURE.as_ref() {
        Some(f) => f,
        None => {
            println!("SKIP: GPU not available");
            return;
        }
    };

    let model_path = match get_inference_model_path() {
        Some(path) => path,
        None => {
            println!("SKIP: No test model found");
            return;
        }
    };

    println!("\n=== Test 12: Full Pipeline E2E (Slow) ===");
    println!("Model: {}", model_path.display());
    println!("GPU: {}", fixture.device_name());

    let start_total = Instant::now();

    // 1. Load model
    println!("\n1. Loading model...");
    let load_start = Instant::now();
    let engine = match create_engine_with_model(&model_path).await {
        Ok(e) => e,
        Err(e) => {
            println!("SKIP: Failed to create engine: {}", e);
            return;
        }
    };
    println!("   Model loaded in {:?}", load_start.elapsed());

    // 2. Run multiple inference requests
    println!("\n2. Running inference requests...");
    let tokenizer = get_tokenizer(&model_path);
    let prompts = vec![
        "The capital of France is",
        "Machine learning is",
        "Rust programming language",
    ];

    let mut total_tokens = 0;

    for (i, prompt) in prompts.iter().enumerate() {
        let inference_start = Instant::now();
        let prompt_tokens = tokenizer.encode(prompt);

        println!(
            "\n  Prompt {}: \"{}\" ({} tokens)",
            i + 1,
            prompt,
            prompt_tokens.len()
        );

        let request_id = match engine
            .submit_request(prompt_tokens.clone(), 15, 0.8, 50, 0.9)
            .await
        {
            Ok(id) => id,
            Err(e) => {
                println!("    ERROR: Failed to submit request: {}", e);
                continue;
            }
        };

        // Wait for completion
        loop {
            let status = match engine.get_request_status(request_id).await {
                Ok(Some(s)) => s,
                Ok(None) => {
                    println!("    ERROR: Request disappeared");
                    break;
                }
                Err(e) => {
                    println!("    ERROR: Failed to get status: {}", e);
                    break;
                }
            };

            if status.is_complete() {
                let text = tokenizer.decode(&status.generated_tokens);
                let count = status.generated_tokens.len();
                total_tokens += count;

                println!(
                    "    Generated {} tokens in {:?}",
                    count,
                    inference_start.elapsed()
                );
                println!("    Text: \"{}\"", text);
                println!(
                    "    Throughput: {:.1} tokens/sec",
                    count as f32 / inference_start.elapsed().as_secs_f32()
                );
                break;
            }

            tokio::time::sleep(Duration::from_millis(50)).await;
        }
    }

    // 3. Print summary
    let total_time = start_total.elapsed();
    println!("\n3. Pipeline Summary:");
    println!("   Total time: {:?}", total_time);
    println!("   Total tokens generated: {}", total_tokens);
    println!(
        "   Average throughput: {:.1} tokens/sec",
        total_tokens as f32 / total_time.as_secs_f32()
    );

    // Clean shutdown
    engine.stop().await.ok();
    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("\nâœ“ Full pipeline E2E test passed\n");
}
